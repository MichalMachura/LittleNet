{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4089,"status":"ok","timestamp":1640208938545,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"__vakEFUF0Ph","outputId":"8aa74194-303e-4109-ce3f-2cd0b5f6870a"},"outputs":[],"source":["!pip3 install brevitas -q\n","# !pip install brevitas -q\n","# %xmode Verbose\n","# !pip install line_profiler -q\n","# %load_ext line_profiler"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3696,"status":"ok","timestamp":1640209221741,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"iQwAEBeiuFtc","outputId":"6c792e41-02a0-4463-8c09-15fb393ea9f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","import brevitas\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import sys\n","import os\n","import cv2 as cv\n","\n","# models_path = '/models_LN5'\n","# models_path = '/models_LN6'\n","# models_path = '/models_LN7'\n","# models_path = '/models_LN7_gciou'\n","models_path = '.'\n","# models_path = '/models_LN7_ciou'\n","# models_path = '/models_LN7_ciou_original'\n","# models_path = '/models_LN7_quant'\n","\n","# dataset_local_path='/media/michal/HDD_Linux_2/DATASETS/Merged_dataset'\n","dataset_local_path='../DATASETS/Merged_dataset'\n","path_to_folds_state = 'folds_state_path_bbox.pkl'\n","local_g_drive = './'\n","\n","# path directory with DAC SDC 2021 - current directory\n","drive_path = local_g_drive\n","# subdirectory with models\n","models_path = drive_path+models_path\n","# path to folds split data and it's state\n","path_to_folds_state = drive_path+path_to_folds_state\n","\n","\"\"\"add path to allow import!!!\"\"\"\n","sys.path.append(drive_path)\n","\n","# get device to execute \n","use_cuda = torch.cuda.is_available()\n","# use_cuda = False\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","print(device)\n","\n","import callbacks\n","import criterions\n","import preprocessing\n","import training\n","import utils\n","import metrics\n","import quantizers as quant\n","import networks\n","import schedulers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":726,"status":"ok","timestamp":1640209222454,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"a-AF0Wx4tZMQ","outputId":"5c720b99-f208-4025-dd27-504c0c5d7a85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input shape = torch.Size([8, 3, 112, 208])\n","Result shape = torch.Size([8, 15, 7, 13])\n","Anchors: \n","[[ 7.247059  10.725    ]\n"," [ 1.6470588  3.25     ]\n"," [ 4.9411764  1.625    ]]\n","Output sizes: \n","[13  7]\n","Number of parameters: \n","243919\n"]}],"source":["\n","# image_shape = (100, 200, 3)\n","image_shape = (112, 208, 3)\n","\n","after_load = preprocessing.numpy_to_torch_iou_params(device)\n","to_anchors_single = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,False,False)\n","# to_anchors_multi = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,True,True)\n","to_anchors_multi = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,True,False)\n","# to_anchors_single = to_anchors_multi\n","\n","anchors = [22,33,\n","            5,10,\n","            15,5\n","          ]\n","anchors = np.array(anchors, np.float32).reshape((-1,2))\n","\n","# normalize anchors\n","anchors *= np.array([[image_shape[0]/340, image_shape[1]/640]])\n","\n","qwi8 = quant.generalized_auto_fxp(bit_width=8, frac_part=8, signed=0.1, \n","                                max_bit_width=8, min_bit_width=8, round_mode='floor',\n","                                trainable_signed=False, trainable_bit_width=False,\n","                                trainable_scale=False, dst='act')\n","qw8 = quant.generalized_auto_fxp(bit_width=8,frac_part=5, signed=0.9,\n","                                min_bit_width=4, max_bit_width=8, \n","                                trainable_signed=False, trainable_bit_width=False,\n","                                dst='weight',)\n","qb8 = quant.generalized_auto_fxp(bit_width=8,frac_part=5, signed=0.9,\n","                                min_bit_width=4, max_bit_width=8, \n","                                trainable_signed=False, trainable_bit_width=False,\n","                                dst='bias',)\n","qa = quant.generalized_auto_fxp(bit_width=8,frac_part=4, signed=0.9,\n","                                round_mode='floor',\n","                                min_bit_width=2, max_bit_width=8, \n","                                trainable_signed=False, trainable_bit_width=False,\n","                                dst='act',)\n","quant_input = qwi8\n","quant_medium = (qw8,\n","                None,\n","                qa, # inter quant - before bn\n","                qa,# out quant - afeter \n","                qb8)\n","quant_out = (qw8,None,qa,qa,qb8)\n","\n","networks.SeparableConv2D.QUANT_BEFORE_BN = True\n","# networks.QuantBatchNorm.ALLOW_QUANTIZATION = False\n","networks.QuantBatchNorm.ALLOW_QUANTIZATION = True\n","net = networks.LittleNet7(anchors.shape[0], None, (None,None,None,None), (None,None,None,None), device=device)\n","# net = networks.LittleNet7(anchors.shape[0], quant_input, quant_medium, quant_out, device=device)\n","\n","# pass example tensor\n","torch.cuda.empty_cache()\n","tensor = torch.rand((8,3,)+image_shape[:2]).to(device)\n","print(\"Input shape =\",tensor.shape)\n","with torch.no_grad():\n","    result = net(tensor)\n","\n","print(\"Result shape =\",result.shape)\n","\n","# get yolo paremeters\n","output_sizes = np.array(result.shape[2:][::-1])\n","p_num = networks.get_number_of_params(net)\n","del tensor\n","del result\n","\n","print(\"Anchors: \")\n","print(anchors) \n","print(\"Output sizes: \")\n","print(output_sizes) \n","print(\"Number of parameters: \")\n","print(p_num) \n","\n","# CREATE GENERATORS\n","def numpy_to_tensor(X,y,device=device):\n","    return utils.data_to_tensor_v3(X,y,device)\n","\n","grid_WH2 = image_shape[:2][::-1] // (2*output_sizes)\n","# create Transformer object\n","transform = preprocessing.Transformer(generator=np.random.default_rng(), \n","                            noise=0.02, \n","                            horizontal_flip=(), \n","                            vertical_flip=(), \n","                            rotate=(-15,15), \n","                            # equalize_hist=0.05, \n","                            blur=7,\n","                            scale=(0.8, 1.5),\n","                            translate=((-grid_WH2[0], grid_WH2[0]),\n","                                       (-grid_WH2[1], grid_WH2[1])),\n","                            HSV=0.005,\n","                            LAB=0.005,\n","                            YCrCb=0.005,\n","                            )\n","# create generators without data => generators templates\n","train_generator = preprocessing.YoloDataGenerator(\n","                            dataset_local_path,\n","                            input_shape=image_shape,\n","                            anchors=anchors,\n","                            images_labes=[],# [(img_path, bbox_path),] \n","                            batch_size=64,\n","                            name='TrainGenerator', \n","                            augmentator=transform,\n","                            output_size=output_sizes,\n","                            after_load=after_load,\n","                            # depends on model\n","                            bbox_to_anchors=to_anchors_multi,\n","                            )\n","val_generator = preprocessing.YoloDataGenerator(\n","                            dataset_local_path,\n","                            input_shape=image_shape,\n","                            anchors=anchors,\n","                            images_labes=[], \n","                            batch_size=64,\n","                            name='ValGenerator', \n","                            augmentator=None,\n","                            output_size=output_sizes,\n","                            after_load=after_load,\n","                            bbox_to_anchors=to_anchors_single,\n","                            # bbox_to_anchors=to_anchors_multi,\n","                            )\n","test_generator = preprocessing.YoloDataGenerator(\n","                            dataset_local_path,\n","                            input_shape=image_shape,\n","                            anchors=anchors,\n","                            images_labes=[], \n","                            batch_size=64,\n","                            name='TestGenerator', \n","                            augmentator=None,\n","                            output_size=output_sizes,\n","                            after_load=after_load,\n","                            bbox_to_anchors=to_anchors_single,\n","                            # bbox_to_anchors=to_anchors_multi,\n","                            )\n","\n","# PATHS FORMATS\n","# paths format is (fold_idx, '{}') - to be possible next format\n","checkpoint_format_path = models_path+'/model_chp_fold_{}_epoch_{}.pt'\n","best_format_path = models_path+'/model_best_fold_{}.pt'\n","best_iou_format_path = models_path+'/model_best_iou_fold_{}.pt'\n","last_format_path = models_path+'/model_last_fold_{}.pt'\n","global_last_path = models_path+'/network_model_last.pt' \n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1640192574159,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"9KYN-gpdn9dj","outputId":"730318cc-45c4-41ae-e364-3c83911bf024"},"outputs":[],"source":["# create_trainer_again = True\n","create_trainer_again = False\n","\n","lr = 0.1\n","momentum = 0.9\n","# optimizer = torch.optim.Adadelta(net.parameters(), lr=lr, weight_decay=0.0001)\n","optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n","# optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","\n","if create_trainer_again:\n","    # init all obejcts once\n","\n","    # criterion\n","    loss_criterion = criterions.YoloIOULoss(anchors, \n","                                            input_shape=image_shape, \n","                                            # weights=[20,1],\n","                                            weights=[1,1],\n","                                            iou_mode='gciou')\n","                                            # iou_mode='ciou')\n","    criterion = criterions.Regularization(loss_fcn=loss_criterion,\n","                                          regularization_loss=criterions.mean_sqare_value,\n","                                          weights=[1.0, 0.00001])\n","\n","    # create metrics\n","    iou = metrics.SingleObjectIOUsBasedMetrics(anchors.copy(),image_shape, device=device)\n","    giou, diou, ciou = iou.get_gdciou()\n","\n","    reg_loss_metric = metrics.ProxyAttributeMetric(criterion, 'reg_loss')\n","    loss_validity_metric = metrics.ProxyAttributeMetric(criterion.loss_fcn, 'loss_validity')\n","    loss_iou_metric = metrics.ProxyAttributeMetric(criterion.loss_fcn, 'loss_iou')\n","\n","    # create callbacks\n","    checkpoint = callbacks.Checkpoint(checkpoint_format_path.format(-1,'{}'))\n","    best = callbacks.SaveBest(best_format_path.format(-1),monitored=['loss','val_loss'], multipliers=[-1, -1])\n","    best_iou = callbacks.SaveBest(best_iou_format_path.format(-1),monitored=['val_iou'], multipliers=[1])\n","    last = callbacks.SaveLast(last_format_path.format(-1))\n","    last_global = callbacks.SaveLast(global_last_path)\n","    early_stopping = callbacks.EarlyStopping(filter_size=5, threshold=0.3)\n","    # plot_clb = callbacks.PlotHistory(5)\n","\n","    # create scheduler\n","    scheduler_ = schedulers.LossDependentScheduler(mul=1.3, div=2.0, init_loss=-np.inf, init_lr=lr, lr_min=1e-5, lr_max=1.0)\n","    # scheduler_ = schedulers.BaseScheduler()\n","    # group metrics and callbacks\n","    metrics_dict = {'r_l':reg_loss_metric,'obj_l':loss_validity_metric,'iou_l':loss_iou_metric,'iou': iou, 'giou':giou, 'diou':diou, 'ciou':ciou}\n","    callbacs_list = [checkpoint,\n","                     best,best_iou, \n","                     last,last_global,\n","                     early_stopping,\n","                    #  plot_clb, \n","                     ]\n","\n","    trainer = training.Trainer(net,\n","                            criterion=criterion,\n","                            optimizer=optimizer,\n","                            name='Trainer',\n","                            metrics=metrics_dict,\n","                            callbacks=callbacs_list,\n","                            checkpoint_clb=checkpoint,\n","                            best_clb=best,\n","                            best_iou_clb=best_iou,\n","                            last_clb=last,\n","                            last_global_clb=last_global,\n","                            early_stopping=early_stopping,\n","                            # plotter=plot_clb,\n","                            info=['LittleNet7 quant, gciou, with input '+str(image_shape)],\n","                            anchors_wh=anchors,\n","                            scheduler=scheduler_,\n","                            fold_state=-1)\n","\n","    torch.save(trainer.get_state(), global_last_path)\n","    \n","    print(\"Trainer object created and saved at\", global_last_path)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"M1NOUYqll92t"},"outputs":[],"source":["# SPLIT DATASET ONTO TESTSET and TRAIN-VAL FOLDS\n","# DO IT ONCE\n","split_again = False\n","\n","if split_again:\n","    # get and split data\n","    dataset = utils.get_all_imgs_gts(dataset_local_path)\n","    training_dataset, test_dataset = utils.train_test_split(dataset, 0.4)\n","\n","    # create folds split for cross validation\n","    folds_of_dicts = utils.split_into_folds(training_dataset, 5)\n","    folds = utils.folds_dict_to_folds_list(folds_of_dicts)\n","    # train set for final evaluation\n","    test_set = utils.dataset_to_list(test_dataset)\n","        \n","    # create folds wrapper \n","    folds_state = training.FoldsState(folds=folds, state=0, test_set=test_set)\n","\n","    training.save_folds(folds_state, path_to_folds_state)\n","\n","    print(\"Data splited into folds\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# d = {'folds':{},\n","#      'test':[]}\n","# for idx, f in enumerate(folds_state.folds):\n","#     Ld = {i:{'path':p,'bbox':dict(zip(['l','t','r','b'],np.round(utils.xcycwh_to_ltrb(bbox.copy().reshape(1,4))).flatten().astype(int).tolist()))} for i, (p,bbox) in enumerate(f)}\n","#     d['folds'][idx] = Ld\n","#     pass\n","    \n","# d['test'] = {i:{'path':p,'bbox':dict(zip(['l','t','r','b'],np.round(utils.xcycwh_to_ltrb(bbox.copy().reshape(1,4))).flatten().astype(int).tolist()))} for i, (p,bbox) in enumerate(folds_state.test_set)}\n","\n","# for k,v in d['folds'].items():\n","#     print(k, len(v))\n","#     print(v[0])\n","# print(d['test'][0])\n","\n","# import json\n","\n","# with open('splited_data_set.json', 'w') as f:\n","#     as_str = json.dumps(d,indent=True)\n","#     f.write(as_str)\n","\n","    "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":773,"status":"ok","timestamp":1640192578488,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"9fx52Y0trlSy","outputId":"bec2eaca-c4ce-47f4-8d82-dc1f7bea857e"},"outputs":[{"name":"stdout","output_type":"stream","text":["<All keys matched successfully>\n","dict_keys(['checkpoint_clb', 'best_clb', 'best_iou_clb', 'last_clb', 'last_global_clb', 'early_stopping', 'info', 'anchors_wh', 'scheduler', 'fold_state'])\n"]}],"source":["# load state \n","folds_state = training.load_folds(path_to_folds_state)\n","# state_dict = torch.load(global_last_path, map_location=torch.device(device))\n","state_dict = torch.load('../trained_models/float_gciou.pt', map_location=torch.device(device))\n","# state_dict = torch.load(best_format_path.format(0), map_location=torch.device(device))\n","# state_dict = torch.load('../network_model_last.pt', map_location=torch.device(device))\n","# state_dict = torch.load(checkpoint_format_path.format(0, 69), map_location=torch.device(device))\n","trainer = training.load_trainer(net, optimizer, state_dict, device)\n","# we = state_dict['model']\n","# trainer.additional_state['scheduler'] = schedulers.LossDependentScheduler(mul=1.3, div=2.0, init_loss=-np.inf, init_lr=lr, lr_min=1e-4, lr_max=1.0)\n","# trainer.criterion.loss_fcn.weights = [1.0,2.0]\n","# trainer.criterion.loss_fcn.input_shape = torch.tensor(image_shape)\n","# trainer.metrics['iou'].img_shape = image_shape\n","# trainer.criterion.regularization_loss = criterions.NearestQuantLoss(8, 2, signed=True, narrow_range=False)\n","# trainer.criterion.regularization_loss.norm = torch.abs\n","# trainer.criterion.regularization_loss.reduction = torch.max\n","\n","# trainer.criterion.loss_fcn.iou_mode = 'gciou'\n","# # additional notes\n","# trainer.additional_state['info'].append(str(trainer.epoch)+' - scheduler LD')\n","# trainer.additional_state['info'] = trainer.additional_state['info'][:-1]\n","# trainer.additional_state['info'].append(str(trainer.epoch)+' - ciou alpha always 1 validity loss from 1 to 100')\n","# torch.save(trainer.get_state(), global_last_path)\n","# trainer.additional_state['best_iou_quant_clb'].best *= 0\n","\n","# trainer.additional_state['info'].append(str(trainer.epoch)+' - start trainable fixedpoint quantization')\n","print(trainer.additional_state.keys())\n","# print(trainer.additional_state['info'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test_generator.images_labes = folds_state.test_set\n","# test_generator.batch_size = 1\n","\n","# for i in range(min(200,len(test_generator))):\n","#     img, y, bbox = test_generator.__getitem__(i,True)\n","#     img = img[0,...].cpu().numpy()\n","#     img = img.reshape((3,-1)).T.reshape((112,208,3)).copy()*255\n","#     img = img.astype(np.uint8)\n","#     bbox = bbox[0,:]\n","#     print(img.shape)\n","#     img = utils.draw_bbox(img,bbox)\n","#     plt.imshow(img)\n","#     plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# h = {k+'_ciou':v[:43] for k,v in trainer.history.items()}\n","# h.update(trainer.history)\n","# plt.grid(True)\n","# hist = h\n","# keys = [k for k in hist.keys() if 'val_' not in k and '_ciou' not in k ]\n","\n","# for k in keys:\n","#     v1 = hist[k]\n","#     v2 = hist['val_'+k]\n","#     v3 = hist[k+'_ciou']\n","#     v4 = hist['val_'+k+'_ciou']\n","    \n","#     v1 = [v.item() if isinstance(v, torch.Tensor) else v for v in v1]\n","#     v2 = [v.item() if isinstance(v, torch.Tensor) else v for v in v2]\n","#     v3 = [v.item() if isinstance(v, torch.Tensor) else v for v in v3]\n","#     v4 = [v.item() if isinstance(v, torch.Tensor) else v for v in v4]\n","\n","#     size = min(len(v1),len(v2),len(v3),len(v4))\n","#     x = np.arange(0,size)+1\n","\n","#     plt.plot(x, v1[:size], label=k)\n","#     plt.plot(x, v2[:size], label='val_'+k)\n","#     plt.plot(x, v3[:size], label=k+'_ciou')\n","#     plt.plot(x, v4[:size], label='val_'+k+'_ciou')\n","#     plt.legend()\n","#     plt.xlabel(\"Epoch\")\n","#     plt.ylabel(k+' value')\n","#     plt.title(\"History of '{}'\".format(k))\n","#     plt.grid(True)\n","#     plt.show()\n","\n","# utils.plot_history(h)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"executionInfo":{"elapsed":749800,"status":"error","timestamp":1640193330703,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"QDByWyZvaDC2","outputId":"ccd589e6-b699-448e-d250-12eb6bfd00e4"},"outputs":[],"source":["# HERE IS MAIN CROSS-VALIDATION LOOP\n","\n","epochs_limit = 350\n","if device == torch.device('cpu') and False:\n","    train_generator.batch_size = 2\n","    val_generator.batch_size = 4\n","    test_generator.batch_size = 4\n","else:\n","    train_generator.batch_size = 8\n","    # train_generator.batch_size = 8\n","    val_generator.batch_size = 8\n","    test_generator.batch_size = 8\n","\n","update_period = 1 + 48*10 // train_generator.batch_size\n","# update_period = 4\n","\n","# get fold state\n","current_fold = folds_state.state\n","\n","for fold_idx in range(current_fold, len(folds_state)):\n","    # if new fold\n","    if fold_idx != trainer.additional_state['fold_state']:\n","        print(\"FOLD\", fold_idx, \"begin \")\n","        # set fold state\n","        trainer.additional_state['fold_state'] = fold_idx\n","        trainer.reset()\n","        # save reseted model\n","        torch.save(trainer.get_state(), global_last_path)\n","    else:\n","        print(\"FOLD\", fold_idx, \"continue\")\n","\n","    # get fold data split\n","    train_data, val_data = folds_state.__getitem__(fold_idx, train_folds=4)\n","    # set callbacks path apropriate to current fold\n","    trainer.additional_state['checkpoint_clb'].formatable_path = checkpoint_format_path.format(fold_idx, '{}')\n","    trainer.additional_state['best_clb'].path = best_format_path.format(fold_idx)\n","    trainer.additional_state['best_iou_clb'].path = best_iou_format_path.format(fold_idx)\n","    trainer.additional_state['last_clb'].path = last_format_path.format(fold_idx)\n","    trainer.additional_state['last_global_clb'].path = global_last_path\n","    \n","    train_generator.images_labes = train_data\n","    val_generator.images_labes = val_data\n","    test_generator.images_labes = folds_state.test_set\n","\n","    # train model up to epochs limit\n","    hist = trainer.continue_fit(train_generator=train_generator, \n","                                epochs=epochs_limit, \n","                                val_generator=val_generator,\n","                                update_period=update_period)\n","    \n","    # evaluate\n","    loss, metrics = trainer.score(test_generator)\n","    print(\"Test evaluation:\",loss, metrics)\n","    # update fold state and save\n","    folds_state.state += 1\n","    training.save_folds(folds_state, path_to_folds_state)\n","    # save model\n","    torch.save(trainer.get_state(), global_last_path)\n","    print(\"FOLD\", fold_idx, \"finished\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KN7PA-pwgCZC"},"outputs":[],"source":["# compute gciou loss metric history\n","# H = trainer.history.copy()\n","# H = state_dict['history']\n","H = src_dict['history']\n","gciou = np.array(H['ciou']) + np.array(H['giou']) - np.array(H['iou'])\n","H['gciou'] = gciou.tolist()\n","val_gciou = np.array(H['val_ciou']) + np.array(H['val_giou']) - np.array(H['val_iou'])\n","H['val_gciou'] = val_gciou.tolist()\n","\n","# utils.plot_history(H, models_path + \"/8_bit_quant_hist_of_{}.png\")\n","utils.plot_history(H, models_path + \"/LN_smaller_hist_of_{}.png\")\n","utils.plot_history(H)\n","# print(np.max(H['val_iou'][-5:]))\n","# print(np.max(H['iou'][-5:]))\n","# print(src_dict['additional_state']['info'])\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"k42WuTROsacn"},"outputs":[{"name":"stdout","output_type":"stream","text":["Score [████████████████████] [16/16] loss=0.25813 r_l=0.38533 obj_l=0.01868 iou_l=0.23945 iou=0.66819 giou=0.59031 diou=0.61082 ciou=0.61081 time=30[s]:92[ms]]\n","For test_3000: 0.258128598690033 {'r_l': 0.38532575964927673, 'obj_l': 0.018677696585655212, 'iou_l': 0.23944710195064545, 'iou': 0.6681918849945068, 'giou': 0.5903083944320677, 'diou': 0.6108209228515623, 'ciou': 0.6108059921264648}\n"]}],"source":["test_generator.batch_size = 128+64\n","val_generator.batch_size = 128+64\n","\n","# test_generator.images_labes = folds_state.test_set\n","# losses_test, metrics_test = trainer.score(test_generator)\n","# losses_val, metrics_val = trainer.score(val_generator)\n","\n","\n","test_generator.images_labes = folds_state.test_set[:3000]\n","losses_test_3000, metrics_test_3000 = trainer.score(test_generator)\n","\n","# print(\"For val:\",losses_val, metrics_val)\n","# print(\"For test:\",losses_test, metrics_test)\n","print(\"For test_3000:\",losses_test_3000, metrics_test_3000)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Score [████████████████████] [16/16] loss=0.26379 r_l=0.91175 obj_l=0.01679 iou_l=0.24699 iou=0.66180 giou=0.58315 diou=0.60443 ciou=0.60442 time=31[s]:646[ms]\n","For val: 0.26734587816304334 {'r_l': 0.9117961525917053, 'obj_l': 0.01681588776409626, 'iou_l': 0.250522255897522, 'iou': 0.6528222642133054, 'giou': 0.5669894416373907, 'diou': 0.5894029177551819, 'ciou': 0.5893876987160576}\n"]}],"source":["test_generator.images_labes = folds_state.test_set[:3000]\n","test_generator.batch_size = 128+64\n","losses_test_3000, metrics_test_3000 = trainer.score(test_generator)\n","print(\"For val:\",losses_val, metrics_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTHyTbW0Q43y"},"outputs":[],"source":["visual_test = True\n","\n","if visual_test:\n","    net = trainer.model\n","    # gen = val_generator\n","    gen = test_generator\n","    # gen.images_labes = folds_state.test_set\n","    gen.images_labes = dst_name_bbox[:16]\n","    # print(dst_name_bbox[:16])\n","    gen.batch_size = 16\n","    scale = [[640/200,360/100,640/200,360/100]]\n","    # images_labels = [(name_img, name_xml),(name_img, name_xml),(name_img, name_xml),(name_img, name_xml),]\n","    # gen.images_labes = utils.convert_2x_path_into_path_bbox(dataset_local_path,images_labels)\n","    # gen.batch_size = 4\n","\n","    index = np.random.randint(0, len(gen))\n","    imgs, y_refs, bboxs = gen.__getitem__(index, True)\n","\n","    # imgs \n","    np.set_printoptions(edgeitems=100, linewidth=200)\n","    \n","    # net inference\n","    with torch.no_grad():\n","        y_pred = net2(imgs)[0]\n","        # y_pred = net(imgs)\n","        # outs = net2.multioutput_forward(imgs)\n","        # y_pred = outs[-1][0]\n","        # print(y_pred[0,...].detach().numpy())\n","    \n","    # first = y_pred[0,...].reshape(5,3*13*7)\n","    # pos = torch.argmax(first[:1,:]).detach().numpy()\n","    # vals = first[1:,pos].clone().detach().numpy()\n","    # # sigm\n","    # v = vals[:2]\n","    # e = np.exp(v)\n","    # s = e / (e+1)\n","    # xy = np.array([5,1]) + s\n","    # xy_ = xy*np.array([200/13,100/7], dtype=np.float32)\n","\n","    # wh = np.exp(vals[-2:])\n","    # wh_ = wh*anchors[0,:]\n","    # lt = xy_-wh_/2\n","    # rb = lt+wh_\n","    # lt_ = lt*np.array(scale)[0,:2]\n","    # rb_ = rb*np.array(scale)[0,:2]\n","    # print(\"Manually computed:\")\n","    # print(vals)\n","    # print(xy, xy_, wh, wh_)\n","    # print(lt,rb)\n","    # print(lt_,rb_)\n","    # print(lt_.astype(np.int32),rb_.astype(np.int32))\n","\n","\n","\n","    y_pred[:,:3*anchors.shape[0],:,:] = torch.sigmoid(y_pred[:,:3*anchors.shape[0],:,:])\n","\n","    # get bbox of prediction\n","    bbox_pred = utils.yolo_outputs_to_single_bbox_v3(y_pred, torch.tensor(anchors, device=device), image_shape)\n","    bbox_pred = np.round(bbox_pred[:,1:].cpu().detach().numpy().reshape(-1,4)).astype(np.int)\n","    bbox_pred_all = bbox_pred\n","   \n","    # get bbox of y_refs\n","    bbox_ref = utils.ltrb_to_xcycwh(y_refs[0].clone())\n","    # print(bbox_ref)\n","    bbox_ref = np.round(bbox_ref.cpu().detach().numpy().reshape(-1,4)).astype(np.int)\n","    bbox_ref_all = bbox_ref\n","    # draw\n","    for idx in range(0,bboxs.shape[0]):\n","        img = imgs[idx,:,:,:].clone().permute(1,2,0).cpu().detach().numpy()*255\n","        img = img.astype(np.uint8)[:,:,::-1]\n","        # original\n","        img_original_marked = utils.draw_bbox(img.copy(), bboxs[idx,:], color=(0,255,0))\n","        # plt.imshow(img_original_marked)\n","        # plt.show()\n","        bbox_pred = bbox_pred_all[idx,:]\n","        # mark prediction\n","        img_original_marked = utils.draw_bbox(img_original_marked, bbox_pred, color=(0,0,255))\n","        # plt.imshow(img_original_marked)\n","        # cv.imwrite('SO_img.png', img_original_marked[:,:,::-1])\n","        # plt.show()\n","        bbox_ref = bbox_ref_all[idx,:]\n","        # mark reference bbox \n","        # img_original_marked = utils.draw_bbox(img_original_marked, bbox_ref, color=(100,0,0))\n","        plt.imshow(img_original_marked)\n","        plt.show()\n","\n","\n","    # cv.imwrite(name_out, img_original_marked)\n","    plt.gray()\n","    \n","    print(y_refs[0])\n","    ref = (y_refs[0]*torch.tensor(scale)).round()\n","    print(ref)\n","    iou = metrics.iou_based_metric(\n","                                utils.xcycwh_to_ltrb(torch.tensor(np.round(bbox_pred_all.astype(np.float32)*np.array(scale))).floor()),\n","                                ref,\n","                                mode='gdciou')\n","    \n","    for i, (n,m) in enumerate(zip(['iou','giou','diou','ciou'],list(iou))):\n","        print(n,\":\",torch.mean(m).item())\n","\n","        \n","    # IMG = utils.batch_masks_to_img(y_pred, image_shape, anchors)[0]\n","    # plt.imshow(IMG)\n","    # plt.show()\n","    # IMG = utils.batch_masks_to_img(y_refs, image_shape, anchors)[0]\n","    # plt.imshow(IMG)\n","    # plt.show()\n","    # print(y_pred)\n","    # print(y_refs)\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" eval_images/images/img_2999.png {'l': 135, 't': 20, 'r': 199, 'b': 57}}}al_images/images/img_0310.png {'l': 65, 't': 4, 'r': 76, 'b': 36}"]}],"source":["import json\n","# CREATE TEST EVAL SET\n","subset = folds_state.test_set[:3000]\n","d = {}\n","generator = preprocessing.YoloDataGenerator(\n","                            dataset_local_path,\n","                            input_shape=image_shape,\n","                            anchors=anchors,\n","                            images_labes=subset, \n","                            batch_size=1,\n","                            name='Generator', \n","                            augmentator=None,\n","                            output_size=output_sizes,\n","                            after_load=after_load,\n","                            # bbox_to_anchors=to_anchors_single,\n","                            bbox_to_anchors=to_anchors_single,\n","                            )\n","\n","dst_dir_path = '../EVAL/eval_images/'\n","# for i,(path,bbox) in enumerate(subset):\n","for i in range(len(generator)):\n","    img,Y = generator[i]\n","    img = img.reshape(3,-1).numpy().T.reshape((112,208,3))\n","    img = (img*255).astype(np.uint8)\n","    bbox = np.round(Y[0].reshape(-1).numpy()).astype(int).flatten().tolist()\n","    \n","    new_path = 'images/img_'+(str(i).zfill(4))+'.png'\n","    dst = os.path.join(dst_dir_path,new_path)\n","#     bbox = np.round(utils.xcycwh_to_ltrb(bbox.copy().reshape((1,4)))).astype(int).flatten().tolist()\n","    bbox = {k:v for k,v in zip('ltrb',bbox)}\n","    d[str(i)] = {'path':new_path,'bbox':bbox}\n","    print('\\r',dst, bbox,end='')\n","    cv.imwrite(dst, img)\n","    \n","# print(d)\n","with open(os.path.join(dst_dir_path,'gt.json'),'w') as f:\n","    f.write(json.dumps(d, indent=4,))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 3000 / 3000000 30003000/ 30003000 3000\n","0.5697525913912298\n"]}],"source":["import quantizers, json\n","def get_dataset(path):\n","    with open(os.path.join(path,'gt.json')) as f:\n","        ds = json.loads(f.read())\n","    \n","    paths = []\n","    ltrb = []\n","    for v in ds.values():\n","        paths.append(os.path.join(path,v['path']))\n","        bbox = v['bbox']\n","        ltrb.append([bbox['l'],bbox['t'],bbox['r'],bbox['b']])\n","\n","    return paths, ltrb\n","\n","paths, gts = get_dataset('../EVAL/eval_images')\n","bb = np.array(gts).astype(np.float32)\n","bb = utils.ltrb_to_xcycwh(bb)\n","ds = [(p,b) for p,b in zip(paths,bb)]\n","\n","generator = preprocessing.YoloDataGenerator(\n","                            # dataset_local_path,\n","                            '',\n","                            input_shape=image_shape,\n","                            anchors=anchors,\n","                            # images_labes=folds_state.test_set[:1], \n","                            images_labes=ds, \n","                            batch_size=1,\n","                            name='Generator', \n","                            augmentator=None,\n","                            output_size=output_sizes,\n","                            after_load=after_load,\n","                            bbox_to_anchors=to_anchors_single,\n","                            )\n","# folds = training.load_folds(path_to_folds_state)\n","# net.load_state_dict(sd,strict=True)\n","# sd = torch.load('../weights_quant_gciou.pt',map_location=torch.device(device))\n","# net.load_state_dict(sd)\n","networks.SeparableConv2D.QUANT_BEFORE_BN = True\n","\n","iou = metrics.SingleObjectIOUsBasedMetrics(anchors.copy(),image_shape, device=device)\n","\n","generator.batch_size = 1\n","# generator.images_labes = folds_state.test_set[:1]\n","iou_results = 0.0\n","torch.set_printoptions(linewidth=3000,edgeitems=15*13*7)\n","np.set_printoptions(linewidth=3000,edgeitems=15*13*7)\n","\n","with torch.no_grad():\n","    for i in  range(len(generator)):\n","        print('\\r',i,'/',len(generator),end='')\n","        X,Y = generator.__getitem__(i)\n","        y_pred = net(X)\n","        iou_results += iou(y_pred,Y)\n","\n","print('\\r',len(generator),'/',len(generator))\n","iou_results /= len(generator)\n","\n","print(iou_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_generator.images_labes = folds_state.test_set[:300]\n","test_generator.batch_size = 1\n","loss, metrics_ = trainer.score(test_generator)\n","print(\"Test evaluation:\",loss, metrics_)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["97969\n","117409\n","29417\n"]}],"source":["folds_state = training.load_folds(path_to_folds_state)\n","print(len(folds_state.test_set))\n","train, val = folds_state.__getitem__(0, train_folds=4)\n","print(len(train))\n","print(len(val))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"custom_detection_network_training_V3.ipynb","provenance":[]},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.7.3 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
