{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4089,"status":"ok","timestamp":1640208938545,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"__vakEFUF0Ph","outputId":"8aa74194-303e-4109-ce3f-2cd0b5f6870a"},"outputs":[],"source":["!pip3 install brevitas -q\n","# !pip install brevitas -q\n","# %xmode Verbose\n","# !pip install line_profiler -q\n","# %load_ext line_profiler"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3696,"status":"ok","timestamp":1640209221741,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"iQwAEBeiuFtc","outputId":"6c792e41-02a0-4463-8c09-15fb393ea9f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","import brevitas\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import sys\n","import os\n","import cv2 as cv\n","\n","colab = True\n","colab = False\n","unzip = True\n","unzip = False\n","\n","# models_path = '/models_LN5'\n","# models_path = '/models_LN6'\n","# models_path = '/models_LN7'\n","# models_path = '/models_LN7_gciou'\n","models_path = '.'\n","# models_path = '/models_LN7_ciou'\n","# models_path = '/models_LN7_ciou_original'\n","# models_path = '/models_LN7_quant'\n","\n","if colab:\n","    from google.colab import drive\n","    import zipfile\n","\n","    dataset_local_path='/content/Merged_dataset'\n","    path_to_folds_state = '/folds_state_path_bbox.pkl'\n","    local_g_drive = '/content/drive'\n","\n","    # mount g_drive and update paths\n","    drive.mount(local_g_drive)\n","    # path directory with DAC SDC 2021\n","    drive_path = local_g_drive+'/MyDrive/SOD_custom_FINN_VAI/TRAIN'\n","    # subdirectory with models\n","    models_path = drive_path+models_path\n","    # path to folds split data and it's state\n","    path_to_folds_state = drive_path+path_to_folds_state\n","\n","    # unzip files frm g_drive zip\n","    path_to_zip_file = drive_path+'/Merged_dataset.zip'\n","    directory_to_extract_to = dataset_local_path[:-len('/Merged_dataset')]\n","    # Unzip if not unzipped\n","    if unzip and (len([f for f in os.listdir('/content') if os.path.isdir(os.path.join('/content', f))]) < 4 \\\n","        or len(os.listdir(dataset_local_path+'/images'))  < 145):\n","        print(\"unpacking to \",dataset_local_path)\n","        with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n","            zip_ref.extractall(directory_to_extract_to)\n","\n","else:\n","    dataset_local_path='/media/michal/HDD_Linux_2/DATASETS/Merged_dataset'\n","    path_to_folds_state = 'folds_state_path_bbox.pkl'\n","    local_g_drive = './'\n","\n","    # path directory with DAC SDC 2021 - current directory\n","    drive_path = local_g_drive\n","    # subdirectory with models\n","    models_path = drive_path+models_path\n","    # path to folds split data and it's state\n","    path_to_folds_state = drive_path+path_to_folds_state\n","\n","\"\"\"add path to allow import!!!\"\"\"\n","sys.path.append(drive_path)\n","\n","# get device to execute \n","use_cuda = torch.cuda.is_available()\n","# use_cuda = False\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","print(device)\n","\n","import callbacks\n","import criterions\n","import preprocessing\n","import training\n","import utils\n","import metrics\n","import quantizers as quant\n","import networks\n","import schedulers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":726,"status":"ok","timestamp":1640209222454,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"a-AF0Wx4tZMQ","outputId":"5c720b99-f208-4025-dd27-504c0c5d7a85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input shape = torch.Size([8, 3, 112, 208])\n","Result shape = torch.Size([8, 15, 7, 13])\n","Anchors: \n","[[ 7.247059  10.725    ]\n"," [ 1.6470588  3.25     ]\n"," [ 4.9411764  1.625    ]]\n","Output sizes: \n","[13  7]\n","Number of parameters: \n","243919\n"]}],"source":["\n","# image_shape = (100, 200, 3)\n","image_shape = (112, 208, 3)\n","\n","after_load = preprocessing.numpy_to_torch_iou_params(device)\n","to_anchors_single = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,False,False)\n","# to_anchors_multi = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,True,True)\n","to_anchors_multi = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,True,False)\n","# to_anchors_single = to_anchors_multi\n","\n","anchors = [22,33,\n","            5,10,\n","            15,5\n","          ]\n","anchors = np.array(anchors, np.float32).reshape((-1,2))\n","\n","# normalize anchors\n","anchors *= np.array([[image_shape[0]/340, image_shape[1]/640]])\n","\n","qwi8 = quant.generalized_auto_fxp(bit_width=8, frac_part=6, signed=0.1, \n","                                max_bit_width=8, min_bit_width=8, round_mode='floor',\n","                                trainable_signed=False, trainable_bit_width=False,\n","                                trainable_scale=False, dst='act')\n","qw8 = quant.generalized_auto_fxp(bit_width=8,frac_part=5, signed=0.9,\n","                                min_bit_width=4, max_bit_width=8, \n","                                trainable_signed=False, trainable_bit_width=False,\n","                                dst='weight',)\n","qb8 = quant.generalized_auto_fxp(bit_width=8,frac_part=5, signed=0.9,\n","                                min_bit_width=4, max_bit_width=8, \n","                                trainable_signed=False, trainable_bit_width=False,\n","                                dst='bias',)\n","qa = quant.generalized_auto_fxp(bit_width=8,frac_part=4, signed=0.9,\n","                                min_bit_width=2, max_bit_width=8, \n","                                trainable_signed=False, trainable_bit_width=False,\n","                                dst='act',)\n","quant_input = qwi8\n","quant_medium = (qw8,\n","                None,\n","                qa, # inter quant - before bn\n","                qa,# out quant - afeter \n","                qb8)\n","quant_out = (qw8,None,qa,qa,qb8)\n","\n","networks.SeparableConv2D.QUANT_BEFORE_BN = True\n","# networks.QuantBatchNorm.ALLOW_QUANTIZATION = False\n","# networks.QuantBatchNorm.ALLOW_QUANTIZATION = True\n","# net = networks.LittleNet5(anchors.shape[0], None, (None,None,None,None), (None,None,None,None), device=device)\n","# net = networks.LittleNet6(anchors.shape[0], None, (None,None,None,None), (None,None,None,None), device=device)\n","net = networks.LittleNet7(anchors.shape[0], None, (None,None,None,None), (None,None,None,None), device=device)\n","net = networks.LittleNet7(anchors.shape[0], quant_input, quant_medium, quant_out, device=device)\n","\n","# pass example tensor\n","torch.cuda.empty_cache()\n","tensor = torch.rand((8,3,)+image_shape[:2]).to(device)\n","print(\"Input shape =\",tensor.shape)\n","with torch.no_grad():\n","    result = net(tensor)\n","\n","print(\"Result shape =\",result.shape)\n","\n","# get yolo paremeters\n","# output_sizes = net.output_sizes(input_size=image_shape[:2][::-1])[-1,:]\n","output_sizes = np.array(result.shape[2:][::-1])\n","p_num = networks.get_number_of_params(net)\n","del tensor\n","del result\n","\n","print(\"Anchors: \")\n","print(anchors) \n","print(\"Output sizes: \")\n","print(output_sizes) \n","print(\"Number of parameters: \")\n","print(p_num) \n","\n","# CREATE GENERATORS\n","def numpy_to_tensor(X,y,device=device):\n","    return utils.data_to_tensor_v3(X,y,device)\n","(None,None,None,None)\n","grid_WH2 = image_shape[:2][::-1] // (2*output_sizes)\n","# create Transformer object\n","transform = preprocessing.Transformer(generator=np.random.default_rng(), \n","                            noise=0.02, \n","                            horizontal_flip=(), \n","                            vertical_flip=(), \n","                            rotate=(-15,15), \n","                            # equalize_hist=0.05, \n","                            blur=7,\n","                            scale=(0.8, 1.5),\n","                            translate=((-grid_WH2[0], grid_WH2[0]),\n","                                       (-grid_WH2[1], grid_WH2[1])),\n","                            HSV=0.005,\n","                            LAB=0.005,\n","                            YCrCb=0.005,\n","                            )\n","# create generators without data => generators templates\n","train_generator = preprocessing.YoloDataGenerator(\n","                            dataset_local_path,\n","                            input_shape=image_shape,\n","                            anchors=anchors,\n","                            images_labes=[],# [(img_path, bbox_path),] \n","                            batch_size=64,\n","                            name='TrainGenerator', \n","                            augmentator=transform,\n","                            output_size=output_sizes,\n","                            after_load=after_load,\n","                            # depends on model\n","                            bbox_to_anchors=to_anchors_multi,\n","                            )\n","val_generator = preprocessing.YoloDataGenerator(\n","                            dataset_local_path,\n","                            input_shape=image_shape,\n","                            anchors=anchors,\n","                            images_labes=[], \n","                            batch_size=64,\n","                            name='ValGenerator', \n","                            augmentator=None,\n","                            output_size=output_sizes,\n","                            after_load=after_load,\n","                            # bbox_to_anchors=to_anchors_single,\n","                            bbox_to_anchors=to_anchors_multi,\n","                            )\n","test_generator = preprocessing.YoloDataGenerator(\n","                            dataset_local_path,\n","                            input_shape=image_shape,\n","                            anchors=anchors,\n","                            images_labes=[], \n","                            batch_size=64,\n","                            name='TestGenerator', \n","                            augmentator=None,\n","                            output_size=output_sizes,\n","                            after_load=after_load,\n","                            # bbox_to_anchors=to_anchors_single,\n","                            bbox_to_anchors=to_anchors_multi,\n","                            )\n","\n","# PATHS FORMATS\n","# paths format is (fold_idx, '{}') - to be possible next format\n","checkpoint_format_path = models_path+'/model_chp_fold_{}_epoch_{}.pt'\n","best_format_path = models_path+'/model_best_fold_{}.pt'\n","best_iou_format_path = models_path+'/model_best_iou_fold_{}.pt'\n","last_format_path = models_path+'/model_last_fold_{}.pt'\n","global_last_path = models_path+'/network_model_last.pt' \n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1640210031486,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"A6FyUKGT2cww","outputId":"0099cca5-8baf-4569-df9f-a5e75a2498f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Num of params: 243919\n","In__00_0:  18 BRAMS layers=[0] shapes=[3, 112, 208]\n","Blk_01_0:  35 BRAMS layers=[1, 3] shapes=[[6, 112, 208], [8, 56, 104]]\n","Blk_01_1:  23 BRAMS layers=[2, 4] shapes=[[8, 56, 104], [16, 56, 104]]\n","Blk_02_0:  23 BRAMS layers=[5, 7] shapes=[[32, 28, 52], [64, 28, 52]]\n","Blk_02_1:  12 BRAMS layers=[6, 8] shapes=[[32, 28, 52], [64, 14, 26]]\n","Blk_03_0:   6 BRAMS layers=[9, 11] shapes=[[64, 14, 26], [128, 7, 13]]\n","Blk_03_1:  12 BRAMS layers=[10, 12] shapes=[[128, 14, 26], [128, 7, 13]]\n","Blk_04_0:   6 BRAMS layers=[13, 15] shapes=[[256, 7, 13], [256, 7, 13]]\n","Blk_04_1:  12 BRAMS layers=[14, 16] shapes=[[256, 7, 13], [512, 7, 13]]\n","Blk_05_0:   6 BRAMS layers=[17] shapes=[[256, 7, 13]]\n","Blk_05_1:   6 BRAMS layers=[18] shapes=[[256, 7, 13]]\n","Out_06_0:   1 BRAMS layers=[19] shapes=[15, 7, 13]\n","P_ 0: shape=[6, 11]    type=dw parallelism= 1 => 0.5\n","P_ 1: shape=[8, 9]     type=pw parallelism= 8 => 1.5\n","P_ 2: shape=[8, 12]    type=dw parallelism= 1 => 0.5\n","P_ 3: shape=[16, 11]   type=dw parallelism= 1 => 0.5\n","P_ 4: shape=[32, 19]   type=pw parallelism=16 => 2.0\n","P_ 5: shape=[32, 12]   type=dw parallelism= 1 => 0.5\n","P_ 6: shape=[64, 11]   type=dw parallelism= 1 => 0.5\n","P_ 7: shape=[64, 66]   type=pw parallelism=16 => 2.0\n","P_ 8: shape=[64, 12]   type=dw parallelism= 1 => 0.5\n","P_ 9: shape=[128, 12]  type=dw parallelism= 1 => 0.5\n","P_10: shape=[128, 130] type=pw parallelism=16 => 4.0\n","P_11: shape=[128, 12]  type=dw parallelism= 1 => 0.5\n","P_12: shape=[256, 12]  type=dw parallelism= 1 => 0.5\n","P_13: shape=[256, 259] type=pw parallelism=18 => 8.0\n","P_14: shape=[256, 12]  type=dw parallelism= 1 => 0.5\n","P_15: shape=[512, 12]  type=dw parallelism= 1 => 1.0\n","P_16: shape=[256, 515] type=pw parallelism=18 => 16.0\n","P_17: shape=[256, 11]  type=dw parallelism= 1 => 0.5\n","P_18: shape=[15, 257]  type=pw parallelism= 5 => 1.5\n","Total 159.0 BRAM used by BUFFERs\n","Total 41.5 BRAM used by ROMs\n","Total 200.5 BRAM usage\n","Total 321 DSP48 usage\n"]}],"source":["\n","print(\"Num of params:\", networks.get_number_of_params(net))\n","\n","# get layers outputs\n","with torch.no_grad():\n","    t = torch.rand(1,3,*image_shape[:2], device=device)\n","    ans = net.multioutput_forward(t)[:-1]\n","\n","layers_outputs = np.array([list(a.size()) for a in ans])\n","\n","# get params as [ ([filters, filter_len],is_dw, paralellism),...]\n","params = []\n","layers = net.layers[1:-1]\n","\n","for i, L in enumerate(layers):\n","    p = networks.get_number_of_params(L)\n","    is_dw = type(L) is networks.DWConv2d\n","    ch_out = L.out_channels\n","    \n","    if is_dw:\n","        paralellism = 1\n","    elif i+1 == len(layers):\n","        paralellism = 5\n","    else:\n","        ceil = lambda *x: int((x[0]-1)/x[1] +1)\n","        paralellism = 16 if ceil(ch_out,16) == ceil(ch_out,18) else 18\n","\n","    paralellism = min(ch_out, paralellism)\n","\n","    params.append(([ch_out, p//ch_out], is_dw, paralellism))\n","\n","# get buffers BRAMs and assigned layers\n","in_layer,\\\n","blocks_buffers,\\\n","out_layer,\\\n","assignments = utils.buffers_brams_usage(layers_outputs,\n","                                        layers_per2buffers=4, \n","                                    #    separate_out=False\n","                                        separate_out=True\n","                                        )\n","# get ROMs BRAMs\n","blocks_roms = utils.roms_usage(params,bit_width=8)\n","\n","# print buffers\n","name = \"{}_{:02}_{:1}: {:3.0f} BRAMS layers={} shapes={}\"\n","shapes = np.array(layers_outputs)[:,1:]\n","print(name.format(\"In_\", 0, 0, in_layer, [0], shapes[0,:].tolist()))\n","for blk_idx, blk in enumerate(blocks_buffers):\n","    for buff_idx, buff in enumerate(blk):\n","        layers_idx = assignments[blk_idx][buff_idx]\n","        print(name.format(\"Blk\",blk_idx+1, buff_idx, buff, layers_idx, shapes[layers_idx,:].tolist()))\n","print(name.format(\"Out\", len(blocks_buffers)+1, 0, out_layer, [len(layers_outputs)-1],shapes[-1,:].tolist()))\n","\n","# print ROMS\n","templ = 'P_{:2}: shape={:10} type={:2} parallelism={:2} => {:3}'\n","for i, ((shape,is_dw, paralellism),(brams)) in enumerate(zip(params,blocks_roms)):\n","    print(templ.format(i,str(shape),'dw' if is_dw else 'pw', paralellism, brams))\n","\n","tot_roms_bram = sum(blocks_roms)\n","tot_buff_bram = in_layer+np.sum(blocks_buffers) # out_layer\n","\n","print(\"Total {} BRAM used by BUFFERs\".format(tot_buff_bram))\n","print(\"Total {} BRAM used by ROMs\".format(tot_roms_bram))\n","print(\"Total {} BRAM usage\".format(tot_roms_bram+tot_buff_bram))\n","print(\"Total {} DSP48 usage\".format(utils.DSP_cout(params)))"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"m8y53cTIlmOO"},"outputs":[],"source":["# save only weights to file\n","# sd = torch.load('../network_model_last.pt', map_location=torch.device(device))\n","# src_dict = torch.load(global_last_path, map_location=torch.device(device))\n","# utils.plot_history(src_dict['history'])\n","# w = sd['model']\n","# net.load_state_dict(w,strict=False)\n","# torch.save(src_dict['model'], models_path+'/weights_only.pth')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1640192574159,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"9KYN-gpdn9dj","outputId":"730318cc-45c4-41ae-e364-3c83911bf024"},"outputs":[],"source":["# create_trainer_again = True\n","create_trainer_again = False\n","\n","lr = 0.1\n","momentum = 0.9\n","# optimizer = torch.optim.Adadelta(net.parameters(), lr=lr, weight_decay=0.0001)\n","optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n","# optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","\n","if create_trainer_again:\n","    # init all obejcts once\n","\n","    # criterion\n","    loss_criterion = criterions.YoloIOULoss(anchors, \n","                                            input_shape=image_shape, \n","                                            # weights=[20,1],\n","                                            weights=[1,1],\n","                                            iou_mode='gciou')\n","                                            # iou_mode='ciou')\n","    criterion = criterions.Regularization(loss_fcn=loss_criterion,\n","                                          regularization_loss=criterions.mean_sqare_value,\n","                                          weights=[1.0, 0.00001])\n","\n","    # create metrics\n","    iou = metrics.SingleObjectIOUsBasedMetrics(anchors.copy(),image_shape, device=device)\n","    giou, diou, ciou = iou.get_gdciou()\n","\n","    reg_loss_metric = metrics.ProxyAttributeMetric(criterion, 'reg_loss')\n","    loss_validity_metric = metrics.ProxyAttributeMetric(criterion.loss_fcn, 'loss_validity')\n","    loss_iou_metric = metrics.ProxyAttributeMetric(criterion.loss_fcn, 'loss_iou')\n","\n","    # create callbacks\n","    checkpoint = callbacks.Checkpoint(checkpoint_format_path.format(-1,'{}'))\n","    best = callbacks.SaveBest(best_format_path.format(-1),monitored=['loss','val_loss'], multipliers=[-1, -1])\n","    best_iou = callbacks.SaveBest(best_iou_format_path.format(-1),monitored=['val_iou'], multipliers=[1])\n","    last = callbacks.SaveLast(last_format_path.format(-1))\n","    last_global = callbacks.SaveLast(global_last_path)\n","    early_stopping = callbacks.EarlyStopping(filter_size=5, threshold=0.3)\n","    # plot_clb = callbacks.PlotHistory(5)\n","\n","    # create scheduler\n","    scheduler_ = schedulers.LossDependentScheduler(mul=1.3, div=2.0, init_loss=-np.inf, init_lr=lr, lr_min=1e-5, lr_max=1.0)\n","    # scheduler_ = schedulers.BaseScheduler()\n","    # group metrics and callbacks\n","    metrics_dict = {'r_l':reg_loss_metric,'obj_l':loss_validity_metric,'iou_l':loss_iou_metric,'iou': iou, 'giou':giou, 'diou':diou, 'ciou':ciou}\n","    callbacs_list = [checkpoint,\n","                     best,best_iou, \n","                     last,last_global,\n","                     early_stopping,\n","                    #  plot_clb, \n","                     ]\n","\n","    trainer = training.Trainer(net,\n","                            criterion=criterion,\n","                            optimizer=optimizer,\n","                            name='Trainer',\n","                            metrics=metrics_dict,\n","                            callbacks=callbacs_list,\n","                            checkpoint_clb=checkpoint,\n","                            best_clb=best,\n","                            best_iou_clb=best_iou,\n","                            last_clb=last,\n","                            last_global_clb=last_global,\n","                            early_stopping=early_stopping,\n","                            # plotter=plot_clb,\n","                            info=['LittleNet7 quant, gciou, with input '+str(image_shape)],\n","                            anchors_wh=anchors,\n","                            scheduler=scheduler_,\n","                            fold_state=-1)\n","\n","    torch.save(trainer.get_state(), global_last_path)\n","    \n","    print(\"Trainer object created and saved at\", global_last_path)\n"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"M1NOUYqll92t"},"outputs":[],"source":["# SPLIT DATASET ONTO TESTSET and TRAIN-VAL FOLDS\n","# DO IT ONCE\n","split_again = False\n","\n","if split_again:\n","    # get and split data\n","    dataset = utils.get_all_imgs_gts(dataset_local_path)\n","    training_dataset, test_dataset = utils.train_test_split(dataset, 0.4)\n","\n","    # create folds split for cross validation\n","    folds_of_dicts = utils.split_into_folds(training_dataset, 5)\n","    folds = utils.folds_dict_to_folds_list(folds_of_dicts)\n","    # train set for final evaluation\n","    test_set = utils.dataset_to_list(test_dataset)\n","        \n","    # create folds wrapper \n","    folds_state = training.FoldsState(folds=folds, state=0, test_set=test_set)\n","\n","    training.save_folds(folds_state, path_to_folds_state)\n","\n","    print(\"Data splited into folds\")"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0 29417\n","{'path': 'images/CarScale/0230.jpg', 'bbox': {'l': 129, 't': 108, 'r': 424, 'b': 219}}\n","1 29417\n","{'path': 'images/carchase/00002402.jpg', 'bbox': {'l': 162, 't': 102, 'r': 205, 'b': 129}}\n","2 29417\n","{'path': 'images/Mhyang/0237.jpg', 'bbox': {'l': 115, 't': 46, 'r': 180, 'b': 122}}\n","3 29417\n","{'path': 'images/liverRun/00028533.jpg', 'bbox': {'l': 168, 't': 77, 'r': 285, 'b': 141}}\n","4 29158\n","{'path': 'images/sitcom/00002506.jpg', 'bbox': {'l': 374, 't': 40, 'r': 456, 'b': 151}}\n","{'path': 'images/horseride/00011514.jpg', 'bbox': {'l': 517, 't': 323, 'r': 556, 'b': 383}}\n"]}],"source":["# d = {'folds':{},\n","#      'test':[]}\n","# for idx, f in enumerate(folds_state.folds):\n","#     Ld = {i:{'path':p,'bbox':dict(zip(['l','t','r','b'],np.round(utils.xcycwh_to_ltrb(bbox.copy().reshape(1,4))).flatten().astype(int).tolist()))} for i, (p,bbox) in enumerate(f)}\n","#     d['folds'][idx] = Ld\n","#     pass\n","    \n","# d['test'] = {i:{'path':p,'bbox':dict(zip(['l','t','r','b'],np.round(utils.xcycwh_to_ltrb(bbox.copy().reshape(1,4))).flatten().astype(int).tolist()))} for i, (p,bbox) in enumerate(folds_state.test_set)}\n","\n","# for k,v in d['folds'].items():\n","#     print(k, len(v))\n","#     print(v[0])\n","# print(d['test'][0])\n","\n","# import json\n","\n","# with open('splited_data_set.json', 'w') as f:\n","#     as_str = json.dumps(d,indent=True)\n","#     f.write(as_str)\n","\n","    "]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":773,"status":"ok","timestamp":1640192578488,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"9fx52Y0trlSy","outputId":"bec2eaca-c4ce-47f4-8d82-dc1f7bea857e"},"outputs":[{"name":"stdout","output_type":"stream","text":["<All keys matched successfully>\n","dict_keys(['checkpoint_clb', 'best_clb', 'best_iou_clb', 'last_clb', 'last_global_clb', 'early_stopping', 'info', 'anchors_wh', 'scheduler', 'fold_state'])\n","['LittleNet7 quant, gciou, with input (112, 208, 3)']\n"]}],"source":["# load state \n","folds_state = training.load_folds(path_to_folds_state)\n","state_dict = torch.load(global_last_path, map_location=torch.device(device))\n","# state_dict = torch.load(best_format_path.format(0), map_location=torch.device(device))\n","# state_dict = torch.load('../network_model_last.pt', map_location=torch.device(device))\n","# state_dict = torch.load(checkpoint_format_path.format(0, 69), map_location=torch.device(device))\n","trainer = training.load_trainer(net, optimizer, state_dict, device)\n","# we = state_dict['model']\n","# trainer.additional_state['scheduler'] = schedulers.LossDependentScheduler(mul=1.3, div=2.0, init_loss=-np.inf, init_lr=lr, lr_min=1e-4, lr_max=1.0)\n","# trainer.criterion.loss_fcn.weights = [1.0,2.0]\n","# trainer.criterion.loss_fcn.input_shape = torch.tensor(image_shape)\n","# trainer.metrics['iou'].img_shape = image_shape\n","# trainer.criterion.regularization_loss = criterions.NearestQuantLoss(8, 2, signed=True, narrow_range=False)\n","# trainer.criterion.regularization_loss.norm = torch.abs\n","# trainer.criterion.regularization_loss.reduction = torch.max\n","\n","# trainer.criterion.loss_fcn.iou_mode = 'gciou'\n","# # additional notes\n","# trainer.additional_state['info'].append(str(trainer.epoch)+' - scheduler LD')\n","# trainer.additional_state['info'] = trainer.additional_state['info'][:-1]\n","# trainer.additional_state['info'].append(str(trainer.epoch)+' - ciou alpha always 1 validity loss from 1 to 100')\n","# torch.save(trainer.get_state(), global_last_path)\n","# trainer.additional_state['best_iou_quant_clb'].best *= 0\n","\n","# trainer.additional_state['info'].append(str(trainer.epoch)+' - start trainable fixedpoint quantization')\n","print(trainer.additional_state.keys())\n","print(trainer.additional_state['info'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_generator.images_labes = folds_state.test_set\n","test_generator.batch_size = 1\n","\n","for i in range(min(200,len(test_generator))):\n","    img, y, bbox = test_generator.__getitem__(i,True)\n","    img = img[0,...].cpu().numpy()\n","    img = img.reshape((3,-1)).T.reshape((112,208,3)).copy()*255\n","    img = img.astype(np.uint8)\n","    bbox = bbox[0,:]\n","    print(img.shape)\n","    img = utils.draw_bbox(img,bbox)\n","    plt.imshow(img)\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# h = {k+'_ciou':v[:43] for k,v in trainer.history.items()}\n","# h.update(trainer.history)\n","plt.grid(True)\n","hist = h\n","keys = [k for k in hist.keys() if 'val_' not in k and '_ciou' not in k ]\n","\n","for k in keys:\n","    v1 = hist[k]\n","    v2 = hist['val_'+k]\n","    v3 = hist[k+'_ciou']\n","    v4 = hist['val_'+k+'_ciou']\n","    \n","    v1 = [v.item() if isinstance(v, torch.Tensor) else v for v in v1]\n","    v2 = [v.item() if isinstance(v, torch.Tensor) else v for v in v2]\n","    v3 = [v.item() if isinstance(v, torch.Tensor) else v for v in v3]\n","    v4 = [v.item() if isinstance(v, torch.Tensor) else v for v in v4]\n","\n","    size = min(len(v1),len(v2),len(v3),len(v4))\n","    x = np.arange(0,size)+1\n","\n","    plt.plot(x, v1[:size], label=k)\n","    plt.plot(x, v2[:size], label='val_'+k)\n","    plt.plot(x, v3[:size], label=k+'_ciou')\n","    plt.plot(x, v4[:size], label='val_'+k+'_ciou')\n","    plt.legend()\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(k+' value')\n","    plt.title(\"History of '{}'\".format(k))\n","    plt.grid(True)\n","    plt.show()\n","\n","# utils.plot_history(h)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"executionInfo":{"elapsed":749800,"status":"error","timestamp":1640193330703,"user":{"displayName":"Michał Machura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02967330354466203543"},"user_tz":-60},"id":"QDByWyZvaDC2","outputId":"ccd589e6-b699-448e-d250-12eb6bfd00e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["FOLD 0 continue\n","Epoche 88/350 [>                   ] [0/14677] start"]},{"name":"stderr","output_type":"stream","text":["/media/michal/HDD_Linux_2/REMOTE_PROMETHEUS/TRAIN/utils.py:233: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n","To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:584.)\n","  index //= dim\n"]},{"name":"stdout","output_type":"stream","text":["Epoche 88/350 [>                   ] [444/14677] loss=0.70986 r_l=17.96632 obj_l=0.03625 iou_l=0.67343 iou=0.23324 giou=0.09654 diou=0.13604 ciou=0.13113 time=2[min]:33[s]:46[ms]]"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_30988/1114948874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# train model up to epochs limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     hist = trainer.continue_fit(train_generator=train_generator, \n\u001b[0m\u001b[1;32m     46\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                 \u001b[0mval_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/media/michal/HDD_Linux_2/REMOTE_PROMETHEUS/TRAIN/training.py\u001b[0m in \u001b[0;36mcontinue_fit\u001b[0;34m(self, train_generator, epochs, val_generator, update_period)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0;31m# calculate metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mmetrics_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m                 \u001b[0;31m# compute mean loss and metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mupdate_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/media/michal/HDD_Linux_2/REMOTE_PROMETHEUS/TRAIN/training.py\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(metrics, y_predict, labels)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmetrics_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/media/michal/HDD_Linux_2/REMOTE_PROMETHEUS/TRAIN/metrics.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_predicted, y_ref)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou_based_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;31m# calculate mean of iou based metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# return iou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# HERE IS MAIN CROSS-VALIDATION LOOP\n","\n","epochs_limit = 350\n","if device == torch.device('cpu') and False:\n","    train_generator.batch_size = 2\n","    val_generator.batch_size = 4\n","    test_generator.batch_size = 4\n","else:\n","    train_generator.batch_size = 8\n","    # train_generator.batch_size = 8\n","    val_generator.batch_size = 8\n","    test_generator.batch_size = 8\n","\n","update_period = 1 + 48*10 // train_generator.batch_size\n","# update_period = 4\n","\n","# get fold state\n","current_fold = folds_state.state\n","\n","for fold_idx in range(current_fold, len(folds_state)):\n","    # if new fold\n","    if fold_idx != trainer.additional_state['fold_state']:\n","        print(\"FOLD\", fold_idx, \"begin \")\n","        # set fold state\n","        trainer.additional_state['fold_state'] = fold_idx\n","        trainer.reset()\n","        # save reseted model\n","        torch.save(trainer.get_state(), global_last_path)\n","    else:\n","        print(\"FOLD\", fold_idx, \"continue\")\n","\n","    # get fold data split\n","    train_data, val_data = folds_state.__getitem__(fold_idx, train_folds=4)\n","    # set callbacks path apropriate to current fold\n","    trainer.additional_state['checkpoint_clb'].formatable_path = checkpoint_format_path.format(fold_idx, '{}')\n","    trainer.additional_state['best_clb'].path = best_format_path.format(fold_idx)\n","    trainer.additional_state['best_iou_clb'].path = best_iou_format_path.format(fold_idx)\n","    trainer.additional_state['last_clb'].path = last_format_path.format(fold_idx)\n","    trainer.additional_state['last_global_clb'].path = global_last_path\n","    \n","    train_generator.images_labes = train_data\n","    val_generator.images_labes = val_data\n","\n","    # train model up to epochs limit\n","    hist = trainer.continue_fit(train_generator=train_generator, \n","                                epochs=epochs_limit, \n","                                val_generator=val_generator,\n","                                update_period=update_period)\n","    \n","    # evaluate\n","    test_generator.images_labes = folds_state.test_set\n","    loss, metrics = trainer.score(test_generator)\n","    print(\"Test evaluation:\",loss, metrics)\n","    # update fold state and save\n","    folds_state.state += 1\n","    training.save_folds(folds_state, path_to_folds_state)\n","    # save model\n","    torch.save(trainer.get_state(), global_last_path)\n","    print(\"FOLD\", fold_idx, \"finished\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_32sH24EtbD"},"outputs":[],"source":["# testset copy to drive\n","import pickle\n","# from shutil import copyfile as cpyf\n","# # copy test files to separate dir\n","# test_img_paths = folds_state.test_set\n","# dst_name_bbox = []\n","# idx = 1\n","# for p, bbox in test_img_paths:\n","#     src = dataset_local_path+'/'+p\n","#     img_name = 'img_{:06}.jpg'.format(idx)\n","#     dst = drive_path+'/testset/'+img_name\n","\n","#     dst_bbox = bbox.copy()\n","#     dst_bbox[:2] -= dst_bbox[-2:]/2\n","#     dst_bbox[-2:] += dst_bbox[:2]\n","\n","#     dst_name_bbox.append((img_name, dst_bbox.astype(np.int).tolist()))\n","\n","#     # copy file\n","#     # cpyf(src,dst)\n","#     idx += 1\n","\n","# print(dst_name_bbox[:16])\n","# with open(drive_path+'/testset/ref_file.pkl','wb') as f:\n","#     pickle.dump(dst_name_bbox, f)\n","with open(drive_path+'/testset/ref_file.pkl','rb') as f:\n","    dst_name_bbox = pickle.load(f) \n","\n","dst = []\n","for p, b in dst_name_bbox:\n","    b1 = np.array(b).astype(np.float32)\n","    wh = b1[-2:] - b1[:2]\n","    b1[:2] += wh/2\n","    b1[-2:] = wh\n","    dst.append((p,b1))\n","dst_name_bbox = dst"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDe7GD8ZZTE5"},"outputs":[],"source":["test_generator.images_labes = dst_name_bbox[:16]\n","test_generator.batch_size = 16\n","test_generator.path_to_dataset =drive_path+'/testset'\n","# test_generator.batch_size = 8\n","# trainer.score(test_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KN7PA-pwgCZC"},"outputs":[],"source":["# compute gciou loss metric history\n","# H = trainer.history.copy()\n","# H = state_dict['history']\n","H = src_dict['history']\n","gciou = np.array(H['ciou']) + np.array(H['giou']) - np.array(H['iou'])\n","H['gciou'] = gciou.tolist()\n","val_gciou = np.array(H['val_ciou']) + np.array(H['val_giou']) - np.array(H['val_iou'])\n","H['val_gciou'] = val_gciou.tolist()\n","\n","# utils.plot_history(H, models_path + \"/8_bit_quant_hist_of_{}.png\")\n","utils.plot_history(H, models_path + \"/LN_smaller_hist_of_{}.png\")\n","utils.plot_history(H)\n","# print(np.max(H['val_iou'][-5:]))\n","# print(np.max(H['iou'][-5:]))\n","# print(src_dict['additional_state']['info'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u5VyiWjWyIfK"},"outputs":[],"source":["trainer.metrics['iou'].img_shape = image_shape\n","trainer.criterion.loss_fcn.input_shape = torch.tensor(list(image_shape), dtype=torch.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k42WuTROsacn"},"outputs":[],"source":["losses, metrics_ = trainer.score(val_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTHyTbW0Q43y"},"outputs":[],"source":["visual_test = True\n","\n","if visual_test:\n","\n","    net = trainer.model\n","    # gen = val_generator\n","    gen = test_generator\n","    # gen.images_labes = folds_state.test_set\n","    gen.images_labes = dst_name_bbox[:16]\n","    # print(dst_name_bbox[:16])\n","    gen.batch_size = 16\n","    scale = [[640/200,360/100,640/200,360/100]]\n","    # images_labels = [(name_img, name_xml),(name_img, name_xml),(name_img, name_xml),(name_img, name_xml),]\n","    # gen.images_labes = utils.convert_2x_path_into_path_bbox(dataset_local_path,images_labels)\n","    # gen.batch_size = 4\n","\n","    index = np.random.randint(0, len(gen))\n","    imgs, y_refs, bboxs = gen.__getitem__(index, True)\n","\n","    # imgs \n","    np.set_printoptions(edgeitems=100, linewidth=200)\n","    \n","    # net inference\n","    with torch.no_grad():\n","        y_pred = net2(imgs)[0]\n","        # y_pred = net(imgs)\n","        # outs = net2.multioutput_forward(imgs)\n","        # y_pred = outs[-1][0]\n","        # print(y_pred[0,...].detach().numpy())\n","    \n","    # first = y_pred[0,...].reshape(5,3*13*7)\n","    # pos = torch.argmax(first[:1,:]).detach().numpy()\n","    # vals = first[1:,pos].clone().detach().numpy()\n","    # # sigm\n","    # v = vals[:2]\n","    # e = np.exp(v)\n","    # s = e / (e+1)\n","    # xy = np.array([5,1]) + s\n","    # xy_ = xy*np.array([200/13,100/7], dtype=np.float32)\n","\n","    # wh = np.exp(vals[-2:])\n","    # wh_ = wh*anchors[0,:]\n","    # lt = xy_-wh_/2\n","    # rb = lt+wh_\n","    # lt_ = lt*np.array(scale)[0,:2]\n","    # rb_ = rb*np.array(scale)[0,:2]\n","    # print(\"Manually computed:\")\n","    # print(vals)\n","    # print(xy, xy_, wh, wh_)\n","    # print(lt,rb)\n","    # print(lt_,rb_)\n","    # print(lt_.astype(np.int32),rb_.astype(np.int32))\n","\n","\n","\n","    y_pred[:,:3*anchors.shape[0],:,:] = torch.sigmoid(y_pred[:,:3*anchors.shape[0],:,:])\n","\n","    # get bbox of prediction\n","    bbox_pred = utils.yolo_outputs_to_single_bbox_v3(y_pred, torch.tensor(anchors, device=device), image_shape)\n","    bbox_pred = np.round(bbox_pred[:,1:].cpu().detach().numpy().reshape(-1,4)).astype(np.int)\n","    bbox_pred_all = bbox_pred\n","   \n","    # get bbox of y_refs\n","    bbox_ref = utils.ltrb_to_xcycwh(y_refs[0].clone())\n","    # print(bbox_ref)\n","    bbox_ref = np.round(bbox_ref.cpu().detach().numpy().reshape(-1,4)).astype(np.int)\n","    bbox_ref_all = bbox_ref\n","    # draw\n","    for idx in range(0,bboxs.shape[0]):\n","        img = imgs[idx,:,:,:].clone().permute(1,2,0).cpu().detach().numpy()*255\n","        img = img.astype(np.uint8)[:,:,::-1]\n","        # original\n","        img_original_marked = utils.draw_bbox(img.copy(), bboxs[idx,:], color=(0,255,0))\n","        # plt.imshow(img_original_marked)\n","        # plt.show()\n","        bbox_pred = bbox_pred_all[idx,:]\n","        # mark prediction\n","        img_original_marked = utils.draw_bbox(img_original_marked, bbox_pred, color=(0,0,255))\n","        # plt.imshow(img_original_marked)\n","        # cv.imwrite('SO_img.png', img_original_marked[:,:,::-1])\n","        # plt.show()\n","        bbox_ref = bbox_ref_all[idx,:]\n","        # mark reference bbox \n","        # img_original_marked = utils.draw_bbox(img_original_marked, bbox_ref, color=(100,0,0))\n","        plt.imshow(img_original_marked)\n","        plt.show()\n","\n","\n","    # cv.imwrite(name_out, img_original_marked)\n","    plt.gray()\n","    \n","    print(y_refs[0])\n","    ref = (y_refs[0]*torch.tensor(scale)).round()\n","    print(ref)\n","    iou = metrics.iou_based_metric(\n","                                utils.xcycwh_to_ltrb(torch.tensor(np.round(bbox_pred_all.astype(np.float32)*np.array(scale))).floor()),\n","                                ref,\n","                                mode='gdciou')\n","    \n","    for i, (n,m) in enumerate(zip(['iou','giou','diou','ciou'],list(iou))):\n","        print(n,\":\",torch.mean(m).item())\n","\n","        \n","    # IMG = utils.batch_masks_to_img(y_pred, image_shape, anchors)[0]\n","    # plt.imshow(IMG)\n","    # plt.show()\n","    # IMG = utils.batch_masks_to_img(y_refs, image_shape, anchors)[0]\n","    # plt.imshow(IMG)\n","    # plt.show()\n","    # print(y_pred)\n","    # print(y_refs)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"custom_detection_network_training_V3.ipynb","provenance":[]},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.7.3 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
