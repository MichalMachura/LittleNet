{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install brevitas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xtnQT_6mBq52"
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys, os, shutil, json\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "sys.path.append('..')\n",
    "# from .. \n",
    "import metrics, preprocessing, utils, training\n",
    "import numpy as np\n",
    "\n",
    "preprocessing.BaseGenerator.MAX_NUMBER_OF_THREADS = 2\n",
    "preprocessing.YoloDataGenerator.NUMBER_OF_THREADS = 1\n",
    "\n",
    "metrics.CONSTANTS.OLD_TORCH = True\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dataset_local_path = '../../DATASETS/Merged_dataset'\n",
    "folds = training.load_folds('../folds_state_path_bbox.pkl')\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnchorMul()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DWConv2d(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, intermediate_channels=1, bias=False, use_bn=True, use_relu=False, device=None):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch,in_ch*intermediate_channels,3,padding=1, groups=in_ch, bias=bias)\n",
    "        self.mul = intermediate_channels\n",
    "        self.reordered = False\n",
    "        self.use_bn = use_bn\n",
    "        self.use_relu = use_relu\n",
    "        self.use_bias = bias\n",
    "        \n",
    "        if use_bn:\n",
    "            self.bn = torch.nn.BatchNorm2d(in_ch*intermediate_channels)\n",
    "        else:\n",
    "            self.bn = None\n",
    "        if use_relu:\n",
    "            self.relu = torch.nn.ReLU(True)\n",
    "        else:\n",
    "            self.relu = None\n",
    "    \n",
    "    def reorder(self, order:torch.tensor):\n",
    "        ch_in = self.conv.in_channels\n",
    "        ch_out = self.conv.out_channels\n",
    "        mul = self.mul\n",
    "        \n",
    "        layers = []\n",
    "        indeces = []\n",
    "        for i in range(mul):\n",
    "            ind = torch.arange(0,ch_in)*mul+i\n",
    "            ind = ind[order]\n",
    "            \n",
    "            w = self.conv.weight[ind,...]\n",
    "            b = self.conv.bias[ind,...] if self.use_bias else None\n",
    "            L = DWConv2d(ch_in,intermediate_channels=1, \n",
    "                         bias=self.use_bias,\n",
    "                         use_bn=self.use_bn, \n",
    "                         use_relu=self.use_relu)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                L.conv.weight[...] = w\n",
    "                if self.use_bias:\n",
    "                    L.conv.bias[...] = b\n",
    "            \n",
    "            if self.use_bn is not None:\n",
    "                with torch.no_grad():\n",
    "                    L.bn.weight[...] = self.bn.weight[ind,...]\n",
    "                    L.bn.bias[...] = self.bn.bias[ind,...]\n",
    "                    L.bn.running_mean[...] = self.bn.running_mean[ind,...]\n",
    "                    L.bn.running_var[...] = self.bn.running_var[ind,...]\n",
    "            \n",
    "            indeces.append(ind)\n",
    "            layers.append(L)\n",
    "            self.add_module(\"sub_dw_\"+str(i),L)\n",
    "        \n",
    "        del self.conv\n",
    "        del self.bn\n",
    "        del self.relu\n",
    "        \n",
    "        indeces = torch.cat(indeces)\n",
    "        self.reordered = True\n",
    "        self.layers = layers\n",
    "        \n",
    "        return indeces\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.reordered:\n",
    "            y = []\n",
    "            for L in self.layers:\n",
    "                y.append(L(x))\n",
    "            \n",
    "            x = torch.cat(y,dim=1) if len(y) > 1 else y[0]\n",
    "            \n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "            if self.bn:\n",
    "                x = self.bn(x)\n",
    "            if self.relu:\n",
    "                x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PWConv2d(torch.nn.Module):\n",
    "    def __init__(self,in_ch, out_ch, bias=False, use_bn=True, use_relu=False, use_mp=False, device=None):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch,out_ch,1,padding=0, bias=bias)\n",
    "    \n",
    "        if use_bn:\n",
    "            self.bn = torch.nn.BatchNorm2d(out_ch)\n",
    "        else:\n",
    "            self.bn = None\n",
    "        if use_relu:\n",
    "            self.relu = torch.nn.ReLU(True)\n",
    "        else:\n",
    "            self.relu = None\n",
    "        if use_mp:\n",
    "            self.mp = torch.nn.MaxPool2d(2,2)\n",
    "        else:\n",
    "            self.mp = None\n",
    "    \n",
    "    def reorder(self, order:torch.tensor):\n",
    "        conv = self.conv\n",
    "        ch_in = self.conv.in_channels\n",
    "        ch_out = self.conv.out_channels\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.conv.weight[...] = self.conv.weight[:,order,...] \n",
    "        \n",
    "        return torch.arange(0,ch_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.bn:\n",
    "            x = self.bn(x)\n",
    "        if self.relu:\n",
    "            x = self.relu(x)\n",
    "        if self.mp:\n",
    "            x = self.mp(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class AnchorMul(torch.nn.Module):\n",
    "    def __init__(self, num_of_anchors, device=torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.noa = num_of_anchors\n",
    "        self.anchors = torch.nn.Parameter(data=torch.Tensor(1,2*self.noa,1,1), requires_grad=True)\n",
    "        self.anchors.data.uniform_(-1,1)\n",
    "        self.register_parameter('anchors', self.anchors)\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xvc = x[:,:-2*self.noa,:,:]\n",
    "        xwh = x[:,-2*self.noa:,:,:]\n",
    "        ywh = xwh*torch.exp(self.anchors)\n",
    "        y = torch.cat((xvc,ywh), dim=1)\n",
    "\n",
    "        return y\n",
    "\n",
    "# float LN7\n",
    "net = torch.nn.Sequential(\n",
    "            DWConv2d(3, intermediate_channels=2, bias=False, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(6,8, bias=True, use_bn=True, use_relu=False, use_mp=True, device=device),\n",
    "            DWConv2d(8, bias=True, use_bn=True, use_relu=False, device=device),\n",
    "            DWConv2d(8, intermediate_channels=2, bias=False, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(16,32, bias=True, use_bn=True, use_relu=False, use_mp=True, device=device),\n",
    "            DWConv2d(32, bias=True, use_bn=True, use_relu=False, device=device),\n",
    "            DWConv2d(32, intermediate_channels=2, bias=False, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(64,64, bias=False, use_bn=True, use_relu=False, use_mp=True, device=device),\n",
    "            DWConv2d(64, bias=True, use_bn=True, use_relu=False, device=device),\n",
    "            DWConv2d(64, intermediate_channels=2, bias=True, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(128,128, bias=False, use_bn=True, use_relu=True, use_mp=True, device=device),\n",
    "            DWConv2d(128, bias=True, use_bn=True, use_relu=False, device=device),\n",
    "            DWConv2d(128, intermediate_channels=2, bias=True, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(256,256, bias=True, use_bn=True, use_relu=True, device=device),\n",
    "            DWConv2d(256, bias=True, use_bn=True, use_relu=False, device=device),\n",
    "            DWConv2d(256, intermediate_channels=2, bias=True, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(512,256, bias=True, use_bn=True, use_relu=True, device=device),\n",
    "            DWConv2d(256, intermediate_channels=1, bias=False, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(256,5*3, bias=True, use_bn=False, use_relu=False, device=device)\n",
    ").to(device)\n",
    "anchor_mul = AnchorMul(3,device).to(device)\n",
    "\n",
    "sd = torch.load('weights_float_gciou.pt',map_location=device)\n",
    "k = list(sd.keys())\n",
    "v = list(sd.values())\n",
    "# load anchor mul\n",
    "am_sd = {list(anchor_mul.state_dict().keys())[0]:v[-1]}\n",
    "anchor_mul.load_state_dict(am_sd)\n",
    "# load LN7 weights\n",
    "k = list(net.state_dict().keys()) # net keys\n",
    "net_sd = {k:v for k,v in zip(k,v[:-1])}\n",
    "net.load_state_dict(net_sd)\n",
    "\n",
    "net = net.eval()\n",
    "anchor_mul = anchor_mul.eval()\n",
    "net.train(False)\n",
    "anchor_mul.train(False)\n",
    "\n",
    "# for k,v in net.state_dict().items():\n",
    "#     print(k,v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net\n",
    "order = torch.arange(0,3)\n",
    "for n,m in net.named_children():\n",
    "    order = m.reorder(order)\n",
    "net = net.eval().train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape = torch.Size([8, 3, 112, 208])\n",
      "Result shape = torch.Size([8, 15, 7, 13])\n",
      "Anchors: \n",
      "[[ 7.247059  10.725    ]\n",
      " [ 1.6470588  3.25     ]\n",
      " [ 4.9411764  1.625    ]]\n",
      "Output sizes: \n",
      "[13  7]\n"
     ]
    }
   ],
   "source": [
    "image_shape = (112, 208, 3)\n",
    "\n",
    "after_load = preprocessing.numpy_to_torch_iou_params(device)\n",
    "to_anchors_single = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,False,False)\n",
    "# to_anchors_multi = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,True,True)\n",
    "to_anchors_multi = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,True,False)\n",
    "# to_anchors_single = to_anchors_multi\n",
    "\n",
    "anchors = [22,33,\n",
    "            5,10,\n",
    "            15,5\n",
    "          ]\n",
    "anchors = np.array(anchors, np.float32).reshape((-1,2))\n",
    "anchors *= np.array([[image_shape[0]/340, image_shape[1]/640]])\n",
    "\n",
    "# pass example tensor\n",
    "torch.cuda.empty_cache()\n",
    "tensor = torch.rand((8,3,)+image_shape[:2]).to(device)\n",
    "print(\"Input shape =\",tensor.shape)\n",
    "with torch.no_grad():\n",
    "    result = net(tensor)\n",
    "\n",
    "print(\"Result shape =\",result.shape)\n",
    "\n",
    "# get yolo paremeters\n",
    "# output_sizes = net.output_sizes(input_size=image_shape[:2][::-1])[-1,:]\n",
    "output_sizes = np.array(result.shape[2:][::-1])\n",
    "del tensor\n",
    "del result\n",
    "\n",
    "print(\"Anchors: \")\n",
    "print(anchors) \n",
    "print(\"Output sizes: \")\n",
    "print(output_sizes) \n",
    "\n",
    "# CREATE GENERATORS\n",
    "def numpy_to_tensor(X,y,device=device):\n",
    "    return utils.data_to_tensor_v3(X,y,device)\n",
    "(None,None,None,None)\n",
    "grid_WH2 = image_shape[:2][::-1] // (2*output_sizes)\n",
    "\n",
    "val_generator = preprocessing.YoloDataGenerator(\n",
    "                            dataset_local_path,\n",
    "                            input_shape=image_shape,\n",
    "                            anchors=anchors,\n",
    "                            images_labes=[], \n",
    "                            batch_size=batch_size,\n",
    "                            name='ValGenerator', \n",
    "                            augmentator=None,\n",
    "                            output_size=output_sizes,\n",
    "                            after_load=after_load,\n",
    "                            # bbox_to_anchors=to_anchors_single,\n",
    "                            bbox_to_anchors=to_anchors_multi,\n",
    "                            )\n",
    "test_generator = preprocessing.YoloDataGenerator(\n",
    "                            dataset_local_path,\n",
    "                            input_shape=image_shape,\n",
    "                            anchors=anchors,\n",
    "                            images_labes=[], \n",
    "                            batch_size=batch_size,\n",
    "                            name='TestGenerator', \n",
    "                            augmentator=None,\n",
    "                            output_size=output_sizes,\n",
    "                            after_load=after_load,\n",
    "                            # bbox_to_anchors=to_anchors_single,\n",
    "                            bbox_to_anchors=to_anchors_multi,\n",
    "                            )\n",
    "\n",
    "_, val_set = folds.__getitem__(0, train_folds=4)\n",
    "val_generator.images_labes = val_set\n",
    "test_generator.images_labes = folds.test_set\n",
    "\n",
    "# decorator -> aplly anchor mul before metric calculation \n",
    "metric_iou = metrics.SingleObjectIOUsBasedMetrics(anchors, image_shape, device)\n",
    "def mean_iou(y_pred, y_ref, metric_iou=metric_iou, anchor_mul=anchor_mul):\n",
    "    y_pred = anchor_mul(y_pred)\n",
    "    return metric_iou(y_pred, y_ref)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QFuGuk2NB4fz",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNeeded to run of quantize.\\nfirst with quant_mode = 'calib'\\nsecond with quant_mode = 'test'\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def evaluate(model,\n",
    "             dataloader,\n",
    "             evaluator, fl_model=net\n",
    "             ):\n",
    "    with torch.no_grad():\n",
    "        score = 0.0\n",
    "        cntr = 0\n",
    "        for i in range(len(dataloader)):\n",
    "            XY = dataloader[i]\n",
    "            X = XY[0]\n",
    "            Y = XY[1]\n",
    "            L = X.shape[0]\n",
    "            y_pred = model(X)\n",
    "            score = score*cntr + X.shape[0]*evaluator(y_pred, Y)\n",
    "            cntr += X.shape[0]\n",
    "            score /= cntr\n",
    "            print(\"\\rEvaluation {}/{}. Score = {}\".format(i,len(dataloader), score),end='')\n",
    "        \n",
    "        print(\"\\rEvaluation {}/{}. Score = {}\".format(len(dataloader),len(dataloader), score),end='\\n')\n",
    "\n",
    "\n",
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device,\n",
    "             dataloader,\n",
    "             evaluator):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader target is not needed - for 'calib' must be batch_size == 1\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    # available in docker or after packaging \n",
    "    # vitis-AI-tools/..../pytorch../pytorch_nndct\n",
    "    # and installing the package\n",
    "    from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "    # model to device\n",
    "    model = float_model.to(device)\n",
    "    \n",
    "    # That was present in vai tutorial.\n",
    "    # I don't know if it affects to anything?\n",
    "    # Force to merge BN with CONV for better quantization accuracy\n",
    "    optimize = 1\n",
    "\n",
    "    rand_in = torch.randn((1,)+input_shape[-1:]+input_shape[:2])\n",
    "    print(\"get qunatizer start\")\n",
    "    try:\n",
    "        quantizer = torch_quantizer(\n",
    "            quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\")\n",
    "        print(e)\n",
    "        return\n",
    "    print(\"get qunatizer end\")\n",
    "        \n",
    "    print(\"get quantized model start\")\n",
    "    quantized_model = quantizer.quant_model\n",
    "    print(\"get quantized model end\")\n",
    "\n",
    "    # evaluate\n",
    "    print(\"testing st\")\n",
    "    evaluate(quantized_model, dataloader, evaluator)\n",
    "    print(\"testing end\")\n",
    "\n",
    "    # export config\n",
    "    if quant_mode == 'calib':\n",
    "        print(\"export config\")\n",
    "        quantizer.export_quant_config()\n",
    "        print(\"export config end\")\n",
    "    # export model\n",
    "    if quant_mode == 'test':\n",
    "        print(\"export xmodel\")\n",
    "        quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "        print(\"export xmodel end\")\n",
    "\n",
    "    return\n",
    "\n",
    "\"\"\"\n",
    "Needed to run of quantize.\n",
    "first with quant_mode = 'calib'\n",
    "second with quant_mode = 'test'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate float model on test dataset\n",
    "# evaluate(net,test_generator,evaluator=mean_iou)\n",
    "# Evaluation 6123/6124. Score = 0.6773354439616692\n",
    "# Evaluate float model on val dataset\n",
    "# evaluate(net,val_generator,evaluator=mean_iou)\n",
    "# Evaluation 279/1839. Score = 0.6722009609852523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only subset of val set\n",
    "# set whole dataset\n",
    "val_generator.images_labes = val_set\n",
    "# shuffle samples\n",
    "val_generator.on_epoch_end()\n",
    "# get subset (100) of samples\n",
    "# subset = val_generator.images_labes[:200]\n",
    "val_generator.images_labes = subset\n",
    "# process only one image per forward\n",
    "val_generator.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Parsing Sequential...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quantizable module is generated.(quant_dir/Sequential.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 200/200. Score = 0.6620479585044089\n",
      "testing end\n",
      "export config\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Exporting quant config.(quant_dir/quant_info.json)\u001b[0m\n",
      "export config end\n"
     ]
    }
   ],
   "source": [
    "# Quantize model - calib\n",
    "quantize(net, \n",
    "         image_shape,\n",
    "         quant_dir='quant_dir',\n",
    "         quant_mode='calib',\n",
    "         device=device,\n",
    "         dataloader=val_generator,\n",
    "         evaluator=mean_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Parsing Sequential...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quantizable module is generated.(quant_dir/Sequential.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 6614/97969. Score = 0.6549693154283055XD\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Quantize model - test\n",
    "    quantize(net, \n",
    "             image_shape,\n",
    "             quant_dir='quant_dir',\n",
    "             quant_mode='test',\n",
    "             device=device,\n",
    "             dataloader=val_generator,\n",
    "             evaluator=mean_iou)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "except:\n",
    "    print(\"XD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[UNILOG][INFO] The compiler log will be dumped at \"/tmp/vitis-ai-user/log/xcompiler-20220123-181535-70874\"\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: function\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_CUSTOMIZED\n",
      "[UNILOG][INFO] Graph name: Sequential, with op num: 188\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/TRAIN/Vitis_AI/build/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/TRAIN/Vitis_AI/build/LN7_VAI.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is bb935c9a20bd88579cc1e298a8f0fe1a, and been saved to \"/workspace/TRAIN/Vitis_AI/build/md5sum.txt\"\n"
     ]
    }
   ],
   "source": [
    "!vai_c_xir --xmodel quant_dir/Sequential_int.xmodel --arch arch.json --net_name LN7_VAI --output_dir  build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TEST EVAL SET\n",
    "subset = folds.test_set[:3000]\n",
    "d = {}\n",
    "generator = preprocessing.YoloDataGenerator(\n",
    "                            dataset_local_path,\n",
    "                            input_shape=image_shape,\n",
    "                            anchors=anchors,\n",
    "                            images_labes=subset, \n",
    "                            batch_size=1,\n",
    "                            name='Generator', \n",
    "                            augmentator=None,\n",
    "                            output_size=output_sizes,\n",
    "                            after_load=after_load,\n",
    "                            # bbox_to_anchors=to_anchors_single,\n",
    "                            bbox_to_anchors=to_anchors_single,\n",
    "                            )\n",
    "\n",
    "# for i,(path,bbox) in enumerate(subset):\n",
    "for i in range(len(generator)):\n",
    "    img,Y = generator[i]\n",
    "    img = img.reshape(3,-1).numpy().T.reshape((112,208,3))\n",
    "    img = (img*255).astype(np.uint8)\n",
    "    bbox = np.round(Y[0].reshape(-1).numpy()).astype(int).flatten().tolist()\n",
    "    \n",
    "    new_path = 'images/img_'+(str(i).zfill(4))+'.png'\n",
    "    dst = os.path.join('eval_images/'+new_path)\n",
    "#     bbox = np.round(utils.xcycwh_to_ltrb(bbox.copy().reshape((1,4)))).astype(int).flatten().tolist()\n",
    "    bbox = {k:v for k,v in zip('ltrb',bbox)}\n",
    "    d[str(i)] = {'path':new_path,'bbox':bbox}\n",
    "    cv.imwrite(dst, img)\n",
    "    \n",
    "# print(d)\n",
    "with open('eval_images/gt.json','w') as f:\n",
    "    f.write(json.dumps(d, indent=4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Quantize.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
