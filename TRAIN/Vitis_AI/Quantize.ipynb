{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install brevitas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xtnQT_6mBq52"
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys, os, shutil, json\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "sys.path.append('..')\n",
    "# from .. \n",
    "import metrics, preprocessing, utils, training\n",
    "import numpy as np\n",
    "\n",
    "preprocessing.BaseGenerator.MAX_NUMBER_OF_THREADS = 2\n",
    "preprocessing.YoloDataGenerator.NUMBER_OF_THREADS = 1\n",
    "\n",
    "metrics.CONSTANTS.OLD_TORCH = True\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dataset_local_path = '../../DATASETS/Merged_dataset'\n",
    "folds = training.load_folds('../folds_state_path_bbox.pkl')\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnchorMul()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DWConv2d(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, intermediate_channels=1, bias=False, use_bn=True, use_relu=False, device=None):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch,in_ch*intermediate_channels,3,padding=1, groups=in_ch, bias=bias)\n",
    "        self.mul = intermediate_channels\n",
    "        self.reordered = False\n",
    "        self.use_bn = use_bn\n",
    "        self.use_relu = use_relu\n",
    "        self.use_bias = bias\n",
    "        \n",
    "        if use_bn:\n",
    "            self.bn = torch.nn.BatchNorm2d(in_ch*intermediate_channels)\n",
    "        else:\n",
    "            self.bn = None\n",
    "        if use_relu:\n",
    "            self.relu = torch.nn.ReLU(True)\n",
    "        else:\n",
    "            self.relu = None\n",
    "    \n",
    "    def reorder(self, order:torch.tensor):\n",
    "        ch_in = self.conv.in_channels\n",
    "        ch_out = self.conv.out_channels\n",
    "        mul = self.mul\n",
    "        \n",
    "        layers = []\n",
    "        indeces = []\n",
    "        for i in range(mul):\n",
    "            ind = torch.arange(0,ch_in)*mul+i\n",
    "            ind = ind[order]\n",
    "            \n",
    "            w = self.conv.weight[ind,...]\n",
    "            b = self.conv.bias[ind,...] if self.use_bias else None\n",
    "            L = DWConv2d(ch_in,intermediate_channels=1, \n",
    "                         bias=self.use_bias,\n",
    "                         use_bn=self.use_bn, \n",
    "                         use_relu=self.use_relu)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                L.conv.weight[...] = w\n",
    "                if self.use_bias:\n",
    "                    L.conv.bias[...] = b\n",
    "            \n",
    "            if self.use_bn is not None:\n",
    "                with torch.no_grad():\n",
    "                    L.bn.weight[...] = self.bn.weight[ind,...]\n",
    "                    L.bn.bias[...] = self.bn.bias[ind,...]\n",
    "                    L.bn.running_mean[...] = self.bn.running_mean[ind,...]\n",
    "                    L.bn.running_var[...] = self.bn.running_var[ind,...]\n",
    "            \n",
    "            indeces.append(ind)\n",
    "            layers.append(L)\n",
    "            self.add_module(\"sub_dw_\"+str(i),L)\n",
    "        \n",
    "        del self.conv\n",
    "        del self.bn\n",
    "        del self.relu\n",
    "        \n",
    "        indeces = torch.cat(indeces)\n",
    "        self.reordered = True\n",
    "        self.layers = layers\n",
    "        \n",
    "        return indeces\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.reordered:\n",
    "            y = []\n",
    "            for L in self.layers:\n",
    "                y.append(L(x))\n",
    "            \n",
    "            x = torch.cat(y,dim=1) if len(y) > 1 else y[0]\n",
    "            \n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "            if self.bn:\n",
    "                x = self.bn(x)\n",
    "            if self.relu:\n",
    "                x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PWConv2d(torch.nn.Module):\n",
    "    def __init__(self,in_ch, out_ch, bias=False, use_bn=True, use_relu=False, use_mp=False, device=None):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch,out_ch,1,padding=0, bias=bias)\n",
    "    \n",
    "        if use_bn:\n",
    "            self.bn = torch.nn.BatchNorm2d(out_ch)\n",
    "        else:\n",
    "            self.bn = None\n",
    "        if use_relu:\n",
    "            self.relu = torch.nn.ReLU(True)\n",
    "        else:\n",
    "            self.relu = None\n",
    "        if use_mp:\n",
    "            self.mp = torch.nn.MaxPool2d(2,2)\n",
    "        else:\n",
    "            self.mp = None\n",
    "    \n",
    "    def reorder(self, order:torch.tensor):\n",
    "        conv = self.conv\n",
    "        ch_in = self.conv.in_channels\n",
    "        ch_out = self.conv.out_channels\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.conv.weight[...] = self.conv.weight[:,order,...] \n",
    "        \n",
    "        return torch.arange(0,ch_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.bn:\n",
    "            x = self.bn(x)\n",
    "        if self.relu:\n",
    "            x = self.relu(x)\n",
    "        if self.mp:\n",
    "            x = self.mp(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class AnchorMul(torch.nn.Module):\n",
    "    def __init__(self, num_of_anchors, device=torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.noa = num_of_anchors\n",
    "        self.anchors = torch.nn.Parameter(data=torch.Tensor(1,2*self.noa,1,1), requires_grad=True)\n",
    "        self.anchors.data.uniform_(-1,1)\n",
    "        self.register_parameter('anchors', self.anchors)\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xvc = x[:,:-2*self.noa,:,:]\n",
    "        xwh = x[:,-2*self.noa:,:,:]\n",
    "        ywh = xwh*torch.exp(self.anchors)\n",
    "        y = torch.cat((xvc,ywh), dim=1)\n",
    "\n",
    "        return y\n",
    "\n",
    "# float LN7\n",
    "net = torch.nn.Sequential(\n",
    "            DWConv2d(3, intermediate_channels=2, bias=False, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(6,8, bias=True, use_bn=True, use_relu=False, use_mp=True, device=device),\n",
    "            DWConv2d(8, bias=True, use_bn=True, use_relu=False, device=device),\n",
    "            DWConv2d(8, intermediate_channels=2, bias=False, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(16,32, bias=True, use_bn=True, use_relu=False, use_mp=True, device=device),\n",
    "            DWConv2d(32, bias=True, use_bn=True, use_relu=False, device=device),\n",
    "            DWConv2d(32, intermediate_channels=2, bias=False, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(64,64, bias=False, use_bn=True, use_relu=False, use_mp=True, device=device),\n",
    "            DWConv2d(64, bias=True, use_bn=True, use_relu=False, device=device),\n",
    "            DWConv2d(64, intermediate_channels=2, bias=True, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(128,128, bias=False, use_bn=True, use_relu=True, use_mp=True, device=device),\n",
    "            DWConv2d(128, bias=True, use_bn=True, use_relu=False, device=device),\n",
    "            DWConv2d(128, intermediate_channels=2, bias=True, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(256,256, bias=True, use_bn=True, use_relu=True, device=device),\n",
    "            DWConv2d(256, bias=True, use_bn=True, use_relu=False, device=device),\n",
    "            DWConv2d(256, intermediate_channels=2, bias=True, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(512,256, bias=True, use_bn=True, use_relu=True, device=device),\n",
    "            DWConv2d(256, intermediate_channels=1, bias=False, use_bn=True, use_relu=True, device=device),\n",
    "            PWConv2d(256,5*3, bias=True, use_bn=False, use_relu=False, device=device)\n",
    ").to(device)\n",
    "anchor_mul = AnchorMul(3,device).to(device)\n",
    "\n",
    "sd = torch.load('weights_float_gciou.pt',map_location=device)\n",
    "k = list(sd.keys())\n",
    "v = list(sd.values())\n",
    "# load anchor mul\n",
    "am_sd = {list(anchor_mul.state_dict().keys())[0]:v[-1]}\n",
    "anchor_mul.load_state_dict(am_sd)\n",
    "# load LN7 weights\n",
    "k = list(net.state_dict().keys()) # net keys\n",
    "net_sd = {k:v for k,v in zip(k,v[:-1])}\n",
    "net.load_state_dict(net_sd)\n",
    "\n",
    "net = net.eval()\n",
    "anchor_mul = anchor_mul.eval()\n",
    "net.train(False)\n",
    "anchor_mul.train(False)\n",
    "\n",
    "# for k,v in net.state_dict().items():\n",
    "#     print(k,v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net\n",
    "order = torch.arange(0,3)\n",
    "for n,m in net.named_children():\n",
    "    order = m.reorder(order)\n",
    "net = net.eval().train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape = torch.Size([8, 3, 112, 208])\n",
      "Result shape = torch.Size([8, 15, 7, 13])\n",
      "Anchors: \n",
      "[[ 7.247059  10.725    ]\n",
      " [ 1.6470588  3.25     ]\n",
      " [ 4.9411764  1.625    ]]\n",
      "Output sizes: \n",
      "[13  7]\n"
     ]
    }
   ],
   "source": [
    "image_shape = (112, 208, 3)\n",
    "\n",
    "after_load = preprocessing.numpy_to_torch_iou_params(device)\n",
    "to_anchors_single = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,False,False)\n",
    "# to_anchors_multi = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,True,True)\n",
    "to_anchors_multi = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,True,False)\n",
    "# to_anchors_single = to_anchors_multi\n",
    "\n",
    "anchors = [22,33,\n",
    "            5,10,\n",
    "            15,5\n",
    "          ]\n",
    "anchors = np.array(anchors, np.float32).reshape((-1,2))\n",
    "anchors *= np.array([[image_shape[0]/340, image_shape[1]/640]])\n",
    "\n",
    "# pass example tensor\n",
    "torch.cuda.empty_cache()\n",
    "tensor = torch.rand((8,3,)+image_shape[:2]).to(device)\n",
    "print(\"Input shape =\",tensor.shape)\n",
    "with torch.no_grad():\n",
    "    result = net(tensor)\n",
    "\n",
    "print(\"Result shape =\",result.shape)\n",
    "\n",
    "# get yolo paremeters\n",
    "# output_sizes = net.output_sizes(input_size=image_shape[:2][::-1])[-1,:]\n",
    "output_sizes = np.array(result.shape[2:][::-1])\n",
    "del tensor\n",
    "del result\n",
    "\n",
    "print(\"Anchors: \")\n",
    "print(anchors) \n",
    "print(\"Output sizes: \")\n",
    "print(output_sizes) \n",
    "\n",
    "# CREATE GENERATORS\n",
    "def numpy_to_tensor(X,y,device=device):\n",
    "    return utils.data_to_tensor_v3(X,y,device)\n",
    "(None,None,None,None)\n",
    "grid_WH2 = image_shape[:2][::-1] // (2*output_sizes)\n",
    "\n",
    "val_generator = preprocessing.YoloDataGenerator(\n",
    "                            dataset_local_path,\n",
    "                            input_shape=image_shape,\n",
    "                            anchors=anchors,\n",
    "                            images_labes=[], \n",
    "                            batch_size=batch_size,\n",
    "                            name='ValGenerator', \n",
    "                            augmentator=None,\n",
    "                            output_size=output_sizes,\n",
    "                            after_load=after_load,\n",
    "                            # bbox_to_anchors=to_anchors_single,\n",
    "                            bbox_to_anchors=to_anchors_multi,\n",
    "                            )\n",
    "test_generator = preprocessing.YoloDataGenerator(\n",
    "                            dataset_local_path,\n",
    "                            input_shape=image_shape,\n",
    "                            anchors=anchors,\n",
    "                            images_labes=[], \n",
    "                            batch_size=batch_size,\n",
    "                            name='TestGenerator', \n",
    "                            augmentator=None,\n",
    "                            output_size=output_sizes,\n",
    "                            after_load=after_load,\n",
    "                            # bbox_to_anchors=to_anchors_single,\n",
    "                            bbox_to_anchors=to_anchors_multi,\n",
    "                            )\n",
    "\n",
    "_, val_set = folds.__getitem__(0, train_folds=4)\n",
    "val_generator.images_labes = val_set\n",
    "test_generator.images_labes = folds.test_set[:3000]\n",
    "\n",
    "# decorator -> aplly anchor mul before metric calculation \n",
    "metric_iou = metrics.SingleObjectIOUsBasedMetrics(anchors, image_shape, device)\n",
    "def mean_iou(y_pred, y_ref, metric_iou=metric_iou, anchor_mul=anchor_mul):\n",
    "    y_pred = anchor_mul(y_pred)\n",
    "    return metric_iou(y_pred, y_ref)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QFuGuk2NB4fz",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNeeded to run of quantize.\\nfirst with quant_mode = 'calib'\\nsecond with quant_mode = 'test'\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def evaluate(model,\n",
    "             dataloader,\n",
    "             evaluator, fl_model=net\n",
    "             ):\n",
    "    with torch.no_grad():\n",
    "        score = 0.0\n",
    "        cntr = 0\n",
    "        for i in range(len(dataloader)):\n",
    "            XY = dataloader[i]\n",
    "            X = XY[0]\n",
    "            Y = XY[1]\n",
    "            L = X.shape[0]\n",
    "            y_pred = model(X)\n",
    "            score = score*cntr + X.shape[0]*evaluator(y_pred, Y)\n",
    "            cntr += X.shape[0]\n",
    "            score /= cntr\n",
    "            print(\"\\rEvaluation {}/{}. Score = {}\".format(i,len(dataloader), score),end='')\n",
    "        \n",
    "        print(\"\\rEvaluation {}/{}. Score = {}\".format(len(dataloader),len(dataloader), score),end='\\n')\n",
    "\n",
    "\n",
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device,\n",
    "             dataloader,\n",
    "             evaluator):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader target is not needed - for 'calib' must be batch_size == 1\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    # available in docker or after packaging \n",
    "    # vitis-AI-tools/..../pytorch../pytorch_nndct\n",
    "    # and installing the package\n",
    "    from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "    # model to device\n",
    "    model = float_model.to(device)\n",
    "    \n",
    "    # That was present in vai tutorial.\n",
    "    # I don't know if it affects to anything?\n",
    "    # Force to merge BN with CONV for better quantization accuracy\n",
    "    optimize = 1\n",
    "\n",
    "    rand_in = torch.randn((1,)+input_shape[-1:]+input_shape[:2])\n",
    "    print(\"get qunatizer start\")\n",
    "    try:\n",
    "        quantizer = torch_quantizer(\n",
    "            quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\")\n",
    "        print(e)\n",
    "        return\n",
    "    print(\"get qunatizer end\")\n",
    "        \n",
    "    print(\"get quantized model start\")\n",
    "    quantized_model = quantizer.quant_model\n",
    "    print(\"get quantized model end\")\n",
    "\n",
    "    # evaluate\n",
    "    print(\"testing st\")\n",
    "    evaluate(quantized_model, dataloader, evaluator)\n",
    "    print(\"testing end\")\n",
    "\n",
    "    # export config\n",
    "    if quant_mode == 'calib':\n",
    "        print(\"export config\")\n",
    "        quantizer.export_quant_config()\n",
    "        print(\"export config end\")\n",
    "    # export model\n",
    "    if quant_mode == 'test':\n",
    "        print(\"export xmodel\")\n",
    "#         quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "        print(\"export xmodel end\")\n",
    "\n",
    "    return\n",
    "\n",
    "\"\"\"\n",
    "Needed to run of quantize.\n",
    "first with quant_mode = 'calib'\n",
    "second with quant_mode = 'test'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate float model on test dataset\n",
    "# evaluate(net,test_generator,evaluator=mean_iou)\n",
    "# Evaluation 6123/6124. Score = 0.6773354439616692\n",
    "# Evaluate float model on val dataset\n",
    "# evaluate(net,val_generator,evaluator=mean_iou)\n",
    "# Evaluation 279/1839. Score = 0.6722009609852523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only subset of val set\n",
    "# set whole dataset\n",
    "val_generator.images_labes = val_set\n",
    "# shuffle samples\n",
    "val_generator.on_epoch_end()\n",
    "# get subset (100) of samples\n",
    "subset = val_generator.images_labes[:200]\n",
    "val_generator.images_labes = subset\n",
    "# process only one image per forward\n",
    "val_generator.batch_size = 1\n",
    "test_generator.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Parsing Sequential...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quantizable module is generated.(quant_dir/Sequential.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "[[-21   1   2   7   7]\n",
      " [-23   1   3   6   6]\n",
      " [-21   1   3   3   5]\n",
      " ...\n",
      " [-20  -1  -3   2  19]\n",
      " [-24  -1  -1   6  19]\n",
      " [-22   2  -1  12  20]]\n",
      "tensor([[107.2964,  33.6506, 114.7808,  50.2681],\n",
      "        [ 99.3934,  86.7869, 117.4802, 109.9507],\n",
      "        [ 38.8441,  57.3171,  69.2372,  86.6829],\n",
      "        [102.1905,  36.2750, 105.8095,  47.6437],\n",
      "        [ 65.0237,  30.9621,  78.9763,  56.4317],\n",
      "        [ 17.1510,  43.2652,  60.8594,  68.7348],\n",
      "        [ 52.9883,  29.9519,  67.8852,  69.0993],\n",
      "        [121.6931,  14.1097, 146.3882,  47.9676],\n",
      "        [ 58.8607,  21.3859,  90.8730,  58.6141],\n",
      "        [ 60.7591,  52.7602,  68.4354,  73.3170],\n",
      "        [ 80.3583,   2.5611, 109.2142,  20.8328],\n",
      "        [104.2852,  54.1657, 112.5884,  70.7831],\n",
      "        [110.4537,  60.3649, 120.6727,  73.4728],\n",
      "        [ 68.7801,  43.8330,  84.0935,  53.3615],\n",
      "        [ 56.1278,  51.6593,  82.1385,  74.8231],\n",
      "        [ 76.9024,  45.0953, 109.2600,  66.9047]]) tensor([[107.4289,  34.5333, 115.3578,  51.9556],\n",
      "        [100.5074,  87.3024, 117.1131, 108.5529],\n",
      "        [156.4016,  48.8444, 194.6469,  77.0000],\n",
      "        [101.5211,  37.3333, 104.9414,  46.3556],\n",
      "        [ 65.1875,  28.7000,  79.8547,  54.1333],\n",
      "        [ 12.5849,  42.0630,  62.7700,  66.8220],\n",
      "        [ 53.5469,  20.0667,  67.9812,  63.9333],\n",
      "        [122.1984,  14.6222, 145.8297,  47.6000],\n",
      "        [ 57.6789,  21.3111,  90.3273,  58.9556],\n",
      "        [ 61.1333,  55.0667,  69.0667,  75.1333],\n",
      "        [ 82.1828,   1.6106, 110.1459,  22.9097],\n",
      "        [100.4328,  54.1333, 113.0258,  71.7111],\n",
      "        [109.6448,  60.2104, 120.4719,  73.1580],\n",
      "        [ 68.3305,  43.7635,  84.0826,  52.9433],\n",
      "        [ 56.8063,  51.1000,  81.2516,  74.2000],\n",
      "        [ 76.7839,  44.0020, 108.4627,  67.7367]])\n",
      "Evaluation 0/1839. Score = 0.7599470019340515[[-26   1   1   9   5]\n",
      " [-32   3   1  10   4]\n",
      " [-32   1   2   7   4]\n",
      " ...\n",
      " [-27   0   0  11  20]\n",
      " [-28   1   2  12  21]\n",
      " [-21   0   0  12  30]]\n",
      "tensor([[ 50.3714,  66.0356,  63.6183,  81.8831],\n",
      "        [126.0705,  42.8762, 147.9191,  67.1341],\n",
      "        [ 75.5779,  40.0779, 109.2957,  58.3495],\n",
      "        [ 66.1640,  87.0188,  75.8463, 108.0323],\n",
      "        [ 99.8749,  53.4459, 102.3914,  64.2878],\n",
      "        [ 98.3972,  19.0869, 102.2089,  32.8318],\n",
      "        [ 50.3697,  60.9548,  73.8158,  83.0452],\n",
      "        [ 70.9253,  29.5644,  78.8085,  34.8856],\n",
      "        [ 92.0853,  44.3665,  96.1002,  49.2060],\n",
      "        [ 68.9295,  33.9627,  85.2329,  42.1186],\n",
      "        [100.1348,  40.9295, 103.9465,  50.3330],\n",
      "        [ 42.3505,  41.3391,  49.8350,  60.4986],\n",
      "        [ 44.1103,  49.0711,  53.8125,  64.9185],\n",
      "        [ 85.6216,  52.5110,  92.3681,  61.4787],\n",
      "        [ 81.6469,  39.3810,  96.3428,  51.8815],\n",
      "        [ 86.2171,  56.9909,  93.7016,  69.4914]]) tensor([[ 50.2875,  66.2667,  63.7906,  81.2000],\n",
      "        [131.9111,  44.8000, 147.7778,  72.1778],\n",
      "        [ 74.3184,  40.4721, 107.4104,  58.3613],\n",
      "        [ 64.2086,  87.1111,  76.3352, 110.1333],\n",
      "        [ 99.3445,  53.8222, 101.8320,  64.8667],\n",
      "        [ 96.8570,  19.7556, 100.5883,  33.6000],\n",
      "        [ 50.6828,  62.5333,  80.0664,  83.8444],\n",
      "        [ 71.0492,  29.7111,  77.8898,  34.6889],\n",
      "        [ 91.7266,  45.1111,  95.7688,  49.9333],\n",
      "        [ 66.2331,  34.2854,  83.9898,  43.6991],\n",
      "        [100.1219,  40.7556, 103.6977,  51.1778],\n",
      "        [ 78.7500,  49.0000,  84.0000,  78.9250],\n",
      "        [ 42.2875,  49.6222,  53.7922,  64.5556],\n",
      "        [ 84.8859,  51.4889,  92.5039,  60.5111],\n",
      "        [ 82.1957,  38.4995,  96.3105,  52.2665],\n",
      "        [ 86.2499,  57.0681,  93.8331,  70.1935]])\n",
      "Evaluation 1/1839. Score = 0.7376814782619476[[-17  -2  -1  11   3]\n",
      " [-20  -1   0  11   1]\n",
      " [-16  -1   0   9   0]\n",
      " ...\n",
      " [-28   0   2  11  14]\n",
      " [-27  -1   0  14  17]\n",
      " [-21   0  -1  11  26]]\n",
      "tensor([[165.5780,  38.0266, 170.4220,  43.9631],\n",
      "        [119.2769,  43.1633, 122.7128,  51.3191],\n",
      "        [ 57.0353,  59.9973,  83.0460,  78.2690],\n",
      "        [101.2780,  79.1781, 112.4557,  89.4281],\n",
      "        [ 66.3697,  42.7680,  89.8158,  76.6259],\n",
      "        [100.1618,  21.3427, 107.8382,  39.6061],\n",
      "        [ 50.9663,  45.7699,  80.0849,  62.3114],\n",
      "        [111.4520,  29.9548, 121.1541,  50.0452],\n",
      "        [ 64.4013,  57.5710,  90.8612,  80.6953],\n",
      "        [115.9163,  79.4176, 126.0733,  89.1886],\n",
      "        [ 81.6643,  43.9466, 101.7296,  55.8679],\n",
      "        [ 72.3361,  41.8202,  92.4014,  52.6622],\n",
      "        [ 26.9106,  20.5110,  49.1707,  29.4787],\n",
      "        [ 49.4040,  38.3892,  68.3297,  85.7963],\n",
      "        [ 72.9961,  42.2049,  94.1303,  55.3128],\n",
      "        [101.6544,  64.1289, 106.3456,  69.7087]]) tensor([[165.6208,  37.3613, 170.4326,  43.4481],\n",
      "        [119.4000,  42.4667, 122.8203,  51.3333],\n",
      "        [ 55.6578,  60.3556,  82.2430,  78.5556],\n",
      "        [101.0816,  79.5076, 113.0258,  89.6601],\n",
      "        [ 70.4107,  48.5178,  90.5393,  74.5186],\n",
      "        [ 96.9850,  20.2081, 107.5466,  44.8084],\n",
      "        [ 46.9516,  46.0444,  78.3562,  63.1556],\n",
      "        [111.3156,  30.8000, 120.0219,  48.0667],\n",
      "        [ 62.3938,  53.6667,  95.9187,  81.6667],\n",
      "        [115.0676,  78.5286, 124.5106,  88.0780],\n",
      "        [ 44.7000,  40.6000,  54.4781,  48.0667],\n",
      "        [ 72.0136,  42.6958,  91.6722,  52.8920],\n",
      "        [ 66.2667,  20.7667,  68.6000,  22.8667],\n",
      "        [  7.4092,  39.3104,  28.9680,  90.7433],\n",
      "        [ 72.4699,  41.7662,  94.7128,  53.8067],\n",
      "        [101.2102,  64.5556, 106.1852,  69.8444]])\n",
      "Evaluation 2/1839. Score = 0.7019526759783427[[-18  -1  -1   4   3]\n",
      " [-20   1   0   8   0]\n",
      " [-19  -2  -1   8   0]\n",
      " ...\n",
      " [-23  -1   1  16  21]\n",
      " [-32  -1   2  14  23]\n",
      " [-26   1   0  14  30]]\n",
      "tensor([[ 91.2966,  30.3775, 118.6930,  53.5412],\n",
      "        [122.2070,  73.0188, 131.8703,  99.0625],\n",
      "        [ 89.1022,  80.9693, 104.4155,  93.0411],\n",
      "        [ 92.1504,  70.8015, 105.6872,  84.4610],\n",
      "        [ 96.5579,  43.0346, 101.2797,  58.8030],\n",
      "        [ 93.4715,  56.8999, 100.9560,  63.9737],\n",
      "        [ 63.8483,  38.7524,  80.1517,  50.1212],\n",
      "        [ 95.5658,  45.4933, 103.5606,  66.5067],\n",
      "        [ 27.7170,  48.4314,  44.8891,  57.8349],\n",
      "        [ 28.9551,  60.8644,  43.6510,  88.8693],\n",
      "        [ 80.5274,  20.9065,  89.7388,  25.1039],\n",
      "        [ 77.5423,  38.7970,  96.4680,  50.0766],\n",
      "        [100.9594,  15.7988, 107.0406,  30.2115],\n",
      "        [101.7469,  49.5842, 104.2634,  60.4262],\n",
      "        [118.7567,  83.2983, 125.1620,  92.7017],\n",
      "        [ 76.6745,  39.1307,  86.3767,  67.1356]]) tensor([[ 89.1333,  29.8667, 119.9333,  54.6000],\n",
      "        [121.5766,  71.8667, 131.5266,  98.6222],\n",
      "        [ 88.5799,  81.9010, 104.6692,  92.9886],\n",
      "        [ 91.9836,  69.5084, 105.1628,  83.9990],\n",
      "        [ 98.7227,  36.5556, 103.2312,  50.8667],\n",
      "        [ 94.0586,  56.6222, 101.0547,  63.6222],\n",
      "        [ 62.3938,  37.3333,  73.8792,  49.1556],\n",
      "        [ 96.3968,  44.2353, 103.3244,  67.0880],\n",
      "        [ 30.6627,  49.1479,  48.2107,  57.4090],\n",
      "        [ 76.3159,  73.1990,  88.9462, 112.2333],\n",
      "        [ 80.8919,  20.8367,  89.5584,  24.7973],\n",
      "        [ 80.7813,  40.9117,  93.8979,  47.8707],\n",
      "        [100.8992,  16.4889, 106.3406,  32.0444],\n",
      "        [103.0758,  49.3111, 105.7187,  60.2000],\n",
      "        [118.7781,  83.3778, 125.3078,  92.8667],\n",
      "        [ 76.1797,  38.4222,  87.0625,  65.6444]])\n",
      "Evaluation 3/1839. Score = 0.6952531188726425[[-20  -1   0   7   6]\n",
      " [-19  -4   1   6   4]\n",
      " [-17   0  -1   4   3]\n",
      " ...\n",
      " [-21   2   1  13  15]\n",
      " [-26  -3   0  14  18]\n",
      " [-20  -1  -2  14  22]]\n",
      "tensor([[ 48.5608,  49.2469,  56.0453,  66.6718],\n",
      "        [ 74.7701,  41.9291, 101.2299,  55.5885],\n",
      "        [ 45.6251,  45.0582,  77.6374,  96.9522],\n",
      "        [ 87.4042,  23.1980,  94.3295,  43.2295],\n",
      "        [108.8550,  62.4615, 116.3395,  83.5282],\n",
      "        [103.9547,  22.7136, 111.4392,  42.8041],\n",
      "        [ 50.6626,  34.4409,  55.6037,  47.5488],\n",
      "        [142.4277,  49.7310, 163.5620,  68.0027],\n",
      "        [102.3679,  44.2713, 137.6321,  69.7184],\n",
      "        [ 78.1753,  45.4933,  86.5622,  66.5067],\n",
      "        [110.3269,  24.6735, 116.1006,  40.5209],\n",
      "        [ 37.8884,  52.2891,  55.0604,  87.7922],\n",
      "        [ 93.2721,  68.9169, 101.1553,  77.0728],\n",
      "        [ 82.7107,  86.3254, 113.1038, 109.4891],\n",
      "        [ 47.4647,  33.6145,  58.8015,  72.6518],\n",
      "        [121.3936,  44.3794, 140.4440,  60.2268]]) tensor([[ 48.5333,  50.4000,  55.5333,  66.7333],\n",
      "        [ 74.3696,  42.2212, 100.4446,  55.1600],\n",
      "        [ 49.2333,  48.0667,  87.5000,  85.1667],\n",
      "        [ 87.2180,  23.4889,  93.9031,  42.4667],\n",
      "        [  5.4385,   0.1556, 145.8267,  43.7313],\n",
      "        [104.1641,  21.6222, 111.9375,  41.5333],\n",
      "        [ 52.3930,  35.6222,  55.8133,  49.0000],\n",
      "        [126.8625,  59.1111, 148.0062,  73.8889],\n",
      "        [ 99.8766,  44.2003, 137.4851,  71.2787],\n",
      "        [101.3703,  38.5016, 107.4323,  56.1322],\n",
      "        [111.0047,  24.7333, 116.7570,  39.8222],\n",
      "        [ 37.2500,  53.7600,  54.0125,  86.6133],\n",
      "        [ 93.5922,  69.2222, 101.2102,  77.3111],\n",
      "        [ 78.9700,  87.8337, 113.5124, 112.2333],\n",
      "        [ 46.6531,  31.6447,  59.5748,  73.6338],\n",
      "        [123.0222,  43.1242, 140.2310,  59.8585]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation 4/1839. Score = 0.6876762747764588[[-19   1   0  13   1]\n",
      " [-15   1   0  15   0]\n",
      " [-17  -1   0  14   1]\n",
      " ...\n",
      " [-16   0   2  10  25]\n",
      " [-23   0   2  12  25]\n",
      " [-21   2  -1  13  29]]\n",
      "tensor([[ 81.4979,  41.9650, 105.7646,  55.5527],\n",
      "        [178.1541,  70.5272, 187.8562,  80.8667],\n",
      "        [ 95.4314,  34.7530, 121.4421,  49.1657],\n",
      "        [ 68.1348,  95.9634,  71.9465, 106.3029],\n",
      "        [ 84.5373,  37.3729,  91.4627,  55.5759],\n",
      "        [ 59.9404,  43.4451,  96.2451,  62.8212],\n",
      "        [ 67.0485,  44.1185,  87.1138,  56.6190],\n",
      "        [ 85.3418,  23.1268, 100.8206,  68.1357],\n",
      "        [ 27.5453,  19.7239,  38.8821,  28.2761],\n",
      "        [ 71.7927,  38.6348,  86.6896,  47.0989],\n",
      "        [ 92.0038,  50.0655,  96.9450,  56.2008],\n",
      "        [ 75.7254,  95.6013,  79.5371, 105.0048],\n",
      "        [ 65.1915,  75.1672,  73.0747,  93.4389],\n",
      "        [ 52.6221,  41.3692,  61.3676,  47.5044],\n",
      "        [ 82.6114,  42.2049,  97.3073,  55.3128],\n",
      "        [163.4184,  62.0968, 178.3153,  74.5094]]) tensor([[ 79.8221,  41.8250, 106.2649,  56.3267],\n",
      "        [178.7891,  70.3702, 188.5107,  79.0386],\n",
      "        [ 97.8159,  34.9326, 123.1100,  45.7270],\n",
      "        [ 68.4062,  97.2222,  71.8266, 107.1778],\n",
      "        [ 85.1969,  37.9556,  92.0375,  55.6889],\n",
      "        [ 60.1541,  43.6945,  99.0152,  64.1433],\n",
      "        [ 65.8673,  44.8485,  88.1009,  56.4247],\n",
      "        [ 84.1229,  16.4889,  94.9875,  29.2444],\n",
      "        [ 27.6987,  20.1888,  38.6827,  28.3605],\n",
      "        [ 70.8122,  38.1019,  86.3176,  47.7493],\n",
      "        [ 92.0375,  49.6222,  96.5461,  55.0667],\n",
      "        [ 75.8688,  96.1333,  79.6000, 106.4000],\n",
      "        [ 64.8305,  77.9333,  70.5828,  89.7556],\n",
      "        [ 49.8219,  40.1333,  58.6687,  47.1333],\n",
      "        [ 82.1243,  41.4310,  96.3528,  55.3117],\n",
      "        [ 90.1719,   7.0000,  95.1469,  16.1778]])\n",
      "Evaluation 5/1839. Score = 0.6849878827730814[[-19   1   0  12   5]\n",
      " [-20  -1   1  16   3]\n",
      " [-15  -2   1  17   1]\n",
      " ...\n",
      " [-21   0   1  11  21]\n",
      " [-26  -1   1  12  22]\n",
      " [-23   0   0  12  29]]\n",
      "tensor([[ 38.1973,   1.5191,  71.8131,  24.6433],\n",
      "        [ 72.7084,  17.5618,  81.4539,  21.5646],\n",
      "        [ 76.4380,  54.7524,  97.5723,  66.1212],\n",
      "        [106.3901,  59.2753, 116.0923,  71.7758],\n",
      "        [ 64.5669,  30.7222,  86.8270,  55.0115],\n",
      "        [ 69.2543,  34.8002,  82.1395,  69.8059],\n",
      "        [ 82.4042,  42.5630, 102.4695,  73.3557],\n",
      "        [ 79.6216,  21.9986,  87.5048,  26.0014],\n",
      "        [ 35.5439,  18.2558,  72.5374,  65.6629],\n",
      "        [110.3230,  50.1224, 116.7282,  63.8672],\n",
      "        [ 40.5216,  30.5961,  65.7446,  85.3226],\n",
      "        [ 91.9784,  51.2507,  96.2071,  56.8305],\n",
      "        [104.2810,  75.4503, 129.9853,  91.6761],\n",
      "        [ -1.1466,  18.0360,  15.1569,  40.1264],\n",
      "        [ 75.5708,  41.7727,  90.2668,  51.1761],\n",
      "        [102.8805,  36.4906, 107.1092,  50.9033]]) tensor([[ 36.1744,   0.2333,  70.2838,  24.4790],\n",
      "        [ 73.1154,  17.3802,  81.1595,  21.0930],\n",
      "        [ 75.8200,  54.2710,  97.4390,  65.5760],\n",
      "        [105.5612,  58.2379, 115.9579,  70.2167],\n",
      "        [ 63.7906,  31.5000,  86.3734,  52.9667],\n",
      "        [ 71.2406,  35.4667,  81.0187,  66.2667],\n",
      "        [ 82.1333,  45.7333, 104.3000,  74.4333],\n",
      "        [ 79.5844,  21.0793,  87.5934,  25.2291],\n",
      "        [ 35.0000,  21.2333,  71.6333,  68.1333],\n",
      "        [ 15.9089,  63.5833,  19.2070,  71.5556],\n",
      "        [ 39.4850,  36.9600,  64.4425,  88.1067],\n",
      "        [ 92.1930,  51.3333,  96.7016,  56.7778],\n",
      "        [100.5883,  72.0222, 125.1523,  91.6222],\n",
      "        [  0.2328,  14.9485,  11.4080,  39.1078],\n",
      "        [ 74.9470,  42.3253,  90.4151,  51.5433],\n",
      "        [102.6094,  35.0000, 107.1180,  50.4000]])\n",
      "Evaluation 6/1839. Score = 0.6924147776194981[[-13   0   3   7   2]\n",
      " [-14   0   2   5   0]\n",
      " [-18   1   2   6   1]\n",
      " ...\n",
      " [-21  -2   0  12  19]\n",
      " [-22  -5   1  13  21]\n",
      " [-19   0  -1  14  28]]\n",
      "tensor([[ 18.6364,  82.0627,  55.6299, 107.5098],\n",
      "        [ 57.1098,  25.9276,  92.6239,  73.1236],\n",
      "        [ 79.4647,  49.2442,  90.8016,  57.0221],\n",
      "        [ 87.4936,  42.4588,  94.2401,  65.6225],\n",
      "        [ 75.5453,  41.0638,  86.8821,  47.8098],\n",
      "        [ 90.0936,  43.4361, 110.5125,  66.5743],\n",
      "        [103.2544,  14.3110, 131.0118,  46.6378],\n",
      "        [ 72.1032,  66.3455,  95.0232,  90.6034],\n",
      "        [ 63.3693,  34.4436,  69.3682,  49.4751],\n",
      "        [107.5314,  51.6859, 136.3873,  81.0516],\n",
      "        [ 70.3955,  43.8148,  92.6557,  56.9227],\n",
      "        [163.6950,  37.0745, 168.3863,  42.9255],\n",
      "        [ 92.9199,  39.4185, 102.1313,  63.7079],\n",
      "        [ 64.0892,  39.8354,  88.7843,  60.9021],\n",
      "        [ 96.5129,  51.7530,  99.3016,  64.1657],\n",
      "        [114.5350,  53.9167, 119.7313,  68.2457]]) tensor([[ 14.5536,  81.7927,  56.8160, 109.5990],\n",
      "        [ 56.8063,  27.0667,  92.6594,  73.2667],\n",
      "        [ 79.2191,  48.7363,  90.2870,  56.6347],\n",
      "        [ 87.3797,  43.3829,  94.2806,  65.4531],\n",
      "        [ 73.5920,  41.1833,  87.0998,  48.9953],\n",
      "        [127.4844,  53.2000, 135.5688,  72.1778],\n",
      "        [104.7859,  14.9333, 130.9047,  46.6667],\n",
      "        [ 72.6039,  67.6667,  97.0125,  90.6889],\n",
      "        [ 62.3430,  35.1556,  68.8727,  50.0889],\n",
      "        [107.5844,  51.4889, 137.2789,  81.3556],\n",
      "        [ 69.8764,  43.7607,  93.1623,  56.9240],\n",
      "        [163.5531,  37.2882, 168.2231,  43.4398],\n",
      "        [ 93.8876,  39.9109, 102.5568,  63.7358],\n",
      "        [ 62.8594,  36.1667,  91.0297,  58.3333],\n",
      "        [ 96.7016,  50.5556,  99.3445,  62.3778],\n",
      "        [113.4922,  60.5111, 117.2234,  75.6000]])\n",
      "Evaluation 7/1839. Score = 0.7003581449389458[[-16   0  -1   9   2]\n",
      " [-14   1  -3  12  -2]\n",
      " [-17  -2  -3  12  -2]\n",
      " ...\n",
      " [-25  -1   3   8  18]\n",
      " [-29  -1   2  12  19]\n",
      " [-23   0  -1  10  26]]\n",
      "tensor([[ 71.9547,  42.7058,  79.4392,  48.5567],\n",
      "        [ 77.9674,  47.4487,  98.0327,  62.5617],\n",
      "        [ 89.8890,  55.9205,  97.3735,  69.0284],\n",
      "        [105.4797,  65.7498, 113.7828,  78.2502],\n",
      "        [ 81.8234,  51.9454,  99.9103,  70.2170],\n",
      "        [ 59.5314,  45.8694,  88.3873,  64.1410],\n",
      "        [118.0285,  62.6187, 129.3653,  73.9875],\n",
      "        [ 55.1119,  58.8870,  75.9392,  76.2394],\n",
      "        [ 96.3615,  33.7646, 104.2447,  42.3167],\n",
      "        [ 67.9890,  39.5388,  79.9297,  52.6467],\n",
      "        [ 82.1269,  45.0967, 106.8220,  59.5094],\n",
      "        [114.5851,  51.1162, 118.0210,  58.8941],\n",
      "        [ 50.2578,  39.3836,  61.7422,  68.6977],\n",
      "        [ 87.6995,  32.7838,  98.4629,  60.7887],\n",
      "        [ 86.1483,  39.1307,  98.7253,  67.1356],\n",
      "        [ 93.0834,  65.1104,  98.5650,  74.9709]]) tensor([[ 72.9148,  44.6444,  80.8438,  51.6444],\n",
      "        [ 76.9888,  49.6487,  95.9723,  63.2116],\n",
      "        [ 89.9770,  55.6785,  97.3054,  68.2926],\n",
      "        [105.3539,  65.2483, 113.8384,  77.5152],\n",
      "        [ 82.9084,  51.6211, 102.3420,  70.1322],\n",
      "        [ 58.8601,  46.1755,  84.3905,  61.2490],\n",
      "        [121.0324,  63.3111, 131.9152,  73.2667],\n",
      "        [ 52.1500,  57.6333,  72.4047,  74.9000],\n",
      "        [ 96.7513,  34.9284, 104.1941,  42.6633],\n",
      "        [139.9219,  41.6111, 159.2777,  52.8111],\n",
      "        [ 82.9371,  44.9391, 109.7059,  58.8653],\n",
      "        [112.0930,  50.8667, 115.9797,  57.5556],\n",
      "        [ 94.7333,  19.6000, 109.6667,  36.4000],\n",
      "        [ 87.8974,  33.9438,  98.2368,  62.2347],\n",
      "        [ 50.6297,  73.0497,  62.6394, 102.8603],\n",
      "        [ 93.2812,  64.7111,  99.6555,  73.7333]])\n",
      "Evaluation 8/1839. Score = 0.6865904596116807"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-84488fbd693e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m          \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m          \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m          evaluator=mean_iou)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-398fc19d9297>\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(float_model, input_shape, quant_dir, quant_mode, device, dataloader, evaluator)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing st\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-398fc19d9297>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, evaluator, fl_model)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcntr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mcntr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/TRAIN/Vitis_AI/quant_dir/Sequential.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_62\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_65\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_65\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_67\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_67\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_65\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_68\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_68\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_67\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_69\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_69\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_module_68\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.6/site-packages/pytorch_nndct/nn/modules/relu.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     38\u001b[0m     inputs=[input])\n\u001b[1;32m     39\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_quant_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.6/site-packages/nndct_shared/quantization/utils.py\u001b[0m in \u001b[0;36mpost_quant_process\u001b[0;34m(node, outputs)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             tensor_type='output')\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquant_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.6/site-packages/pytorch_nndct/quantization/torchquantizer.py\u001b[0m in \u001b[0;36mdo_scan\u001b[0;34m(self, res, name, node, tensor_type)\u001b[0m\n\u001b[1;32m    107\u001b[0m           \u001b[0mbit_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m           \u001b[0mrange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m           method = mth)\n\u001b[0m\u001b[1;32m    110\u001b[0m       \u001b[0mbnfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfixpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0;31m# record fix pos of activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.6/site-packages/pytorch_nndct/nn/modules/fix_ops.py\u001b[0m in \u001b[0;36mNndctDiffsFixPos\u001b[0;34m(Tinput, Tbuffer, Tfixpos, bit_width, range, method)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mNndctDiffsFixPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTinput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfixpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbit_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0mdevice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTinput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m   \u001b[0mnndct_kernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiffsFixPos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTinput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfixpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbit_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Quantize model - calib\n",
    "quantize(net, \n",
    "         image_shape,\n",
    "         quant_dir='quant_dir',\n",
    "         quant_mode='calib',\n",
    "         device=device,\n",
    "         dataloader=val_generator,\n",
    "         evaluator=mean_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Parsing Sequential...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quantizable module is generated.(quant_dir/Sequential.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 3000/3000. Score = 0.6580071100730376\n",
      "testing end\n",
      "export xmodel\n",
      "export xmodel end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # Quantize model - test\n",
    "    quantize(net, \n",
    "             image_shape,\n",
    "             quant_dir='quant_dir',\n",
    "             quant_mode='test',\n",
    "             device=device,\n",
    "#              dataloader=val_generator,\n",
    "             dataloader=test_generator,\n",
    "             evaluator=mean_iou)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "except:\n",
    "    print(\"XD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[UNILOG][INFO] The compiler log will be dumped at \"/tmp/vitis-ai-user/log/xcompiler-20220123-181535-70874\"\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: function\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_CUSTOMIZED\n",
      "[UNILOG][INFO] Graph name: Sequential, with op num: 188\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/TRAIN/Vitis_AI/build/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/TRAIN/Vitis_AI/build/LN7_VAI.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is bb935c9a20bd88579cc1e298a8f0fe1a, and been saved to \"/workspace/TRAIN/Vitis_AI/build/md5sum.txt\"\n"
     ]
    }
   ],
   "source": [
    "!vai_c_xir --xmodel quant_dir/Sequential_int.xmodel --arch arch.json --net_name LN7_VAI --output_dir  build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TEST EVAL SET\n",
    "subset = folds.test_set[:3000]\n",
    "d = {}\n",
    "generator = preprocessing.YoloDataGenerator(\n",
    "                            dataset_local_path,\n",
    "                            input_shape=image_shape,\n",
    "                            anchors=anchors,\n",
    "                            images_labes=subset, \n",
    "                            batch_size=1,\n",
    "                            name='Generator', \n",
    "                            augmentator=None,\n",
    "                            output_size=output_sizes,\n",
    "                            after_load=after_load,\n",
    "                            # bbox_to_anchors=to_anchors_single,\n",
    "                            bbox_to_anchors=to_anchors_single,\n",
    "                            )\n",
    "\n",
    "# for i,(path,bbox) in enumerate(subset):\n",
    "for i in range(len(generator)):\n",
    "    img,Y = generator[i]\n",
    "    img = img.reshape(3,-1).numpy().T.reshape((112,208,3))\n",
    "    img = (img*255).astype(np.uint8)\n",
    "    bbox = np.round(Y[0].reshape(-1).numpy()).astype(int).flatten().tolist()\n",
    "    \n",
    "    new_path = 'images/img_'+(str(i).zfill(4))+'.png'\n",
    "    dst = os.path.join('eval_images/'+new_path)\n",
    "#     bbox = np.round(utils.xcycwh_to_ltrb(bbox.copy().reshape((1,4)))).astype(int).flatten().tolist()\n",
    "    bbox = {k:v for k,v in zip('ltrb',bbox)}\n",
    "    d[str(i)] = {'path':new_path,'bbox':bbox}\n",
    "    cv.imwrite(dst, img)\n",
    "    \n",
    "# print(d)\n",
    "with open('eval_images/gt.json','w') as f:\n",
    "    f.write(json.dumps(d, indent=4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Quantize.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
