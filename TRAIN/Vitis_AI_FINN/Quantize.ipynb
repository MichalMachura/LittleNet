{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install brevitas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xtnQT_6mBq52"
   },
   "outputs": [],
   "source": [
    "import sys, os, shutil, json\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import Module, Conv2d, ReLU6, BatchNorm2d, MaxPool2d\n",
    "# from brevitas_examples.bnn_pynq.models.common import CommonWeightQuant, CommonActQuant\n",
    "# import brevitas.nn as qnn \n",
    "# import brevitas.quant as quant \n",
    "# from brevitas.core.restrict_val import RestrictValueType\n",
    "\n",
    "sys.path.append('..')\n",
    "# from .. \n",
    "import training,preprocessing, utils, metrics\n",
    "import numpy as np\n",
    "metrics.CONSTANTS.OLD_TORCH = True\n",
    "metrics.CONSTANTS.BBOX_BORDER_WIDTH = 1\n",
    "\n",
    "preprocessing.YoloDataGenerator.NAIVE_RESIZE = True\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dataset_local_path = '../../DATASETS/Merged_dataset'\n",
    "folds = training.load_folds('../folds_state_path_bbox.pkl')\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clamp(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,ch_in) -> None:\n",
    "        super().__init__()\n",
    "        self.ch_in = ch_in\n",
    "        torch_type = torch.float32\n",
    "        self.c1 = torch.tensor(1,dtype=torch_type)\n",
    "        self.nc1 = torch.tensor(-1,dtype=torch_type)\n",
    "        self.nc2 = torch.tensor(-2,dtype=torch_type)\n",
    "        self.r = torch.nn.ReLU()\n",
    "    \n",
    "    def add_conv(self):\n",
    "        ch_in = ch_out = self.ch_in\n",
    "        self.conv1 = Conv2d(ch_in,ch_out,1,groups=self.ch_in,padding=0,bias=True)\n",
    "        self.conv2 = Conv2d(ch_in,ch_out,1,groups=self.ch_in,padding=0,bias=True)\n",
    "        self.conv3 = Conv2d(ch_in,ch_out,1,groups=self.ch_in,padding=0,bias=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.conv1.weight[:] = 1.0\n",
    "            self.conv1.bias[:] = 1.0\n",
    "            self.conv2.weight[:] = 1.0\n",
    "            self.conv2.bias[:] = -1.0\n",
    "            self.conv3.weight[:] = -1.0\n",
    "            self.conv3.bias[:] = -1.0\n",
    "    \n",
    "    def forward(self,x):\n",
    "        v1 = self.r(self.conv1(x))\n",
    "        v2 = self.r(self.conv2(x))\n",
    "        v3 = self.conv3(v2)\n",
    "        x = v1+v3\n",
    "        \n",
    "        return x\n",
    "\n",
    "# use_brevitas = True\n",
    "use_brevitas = False\n",
    "def blk(ch_in,ch_out,clamp=True,mp=True):\n",
    "    if use_brevitas:\n",
    "        import brevitas.nn as qnn\n",
    "        from brevitas_examples.bnn_pynq.models.common import CommonActQuant, CommonWeightQuant\n",
    "        L = [qnn.QuantConv2d(ch_in,ch_out,3,padding=1,bias=False,weight_quant=CommonWeightQuant, weight_bit_width=8),\n",
    "            BatchNorm2d(ch_out)]\n",
    "        if clamp:\n",
    "            L.append(qnn.QuantIdentity(act_quant=CommonActQuant,bit_width=8))\n",
    "        if mp:\n",
    "            L.append(qnn.QuantMaxPool2d(2,2))\n",
    "        return tuple(L)\n",
    "    else:\n",
    "        L = [Conv2d(ch_in,ch_out,3,padding=1,bias=False),\n",
    "            BatchNorm2d(ch_out)]\n",
    "        if clamp:\n",
    "            L.append(Clamp(ch_out))\n",
    "        if mp:\n",
    "            L.append(MaxPool2d(2,2))\n",
    "        return tuple(L)\n",
    "\n",
    "def input_layer():\n",
    "    if use_brevitas:\n",
    "        import brevitas.nn as qnn\n",
    "        from brevitas_examples.bnn_pynq.models.common import CommonActQuant, CommonWeightQuant\n",
    "        from brevitas.inject.enum import RestrictValueType\n",
    "        \n",
    "        return tuple([qnn.QuantIdentity( # for Q1.7 input format\n",
    "                act_quant=CommonActQuant,\n",
    "                bit_width=8,\n",
    "                min_val=-1.0,\n",
    "                # max_val=1.0,\n",
    "                max_val=1.0,\n",
    "                narrow_range=False,\n",
    "                restrict_scaling_type=RestrictValueType.POWER_OF_TWO)])\n",
    "    else:\n",
    "        return tuple([])\n",
    "\n",
    "# float \n",
    "net = torch.nn.Sequential(\n",
    "    # *input_layer(),\n",
    "    *blk(3,16),\n",
    "    *blk(16,32),\n",
    "    *blk(32,64),\n",
    "    *blk(64,64),\n",
    "    *blk(64,64,mp=False),\n",
    "    *blk(64,30,mp=False,clamp=False),\n",
    "    \n",
    ").to(device)\n",
    "\n",
    "sd = torch.load('best.pt',map_location=device)['model']\n",
    "v1 = list(sd.values())\n",
    "k2 = list(net.state_dict().keys()) # net keys\n",
    "    \n",
    "net_sd = {k:v for k,v in zip(k2,v1)}\n",
    "net.load_state_dict(net_sd)\n",
    "\n",
    "for m in net.modules():\n",
    "    if isinstance(m,Clamp):\n",
    "            m.add_conv()\n",
    "net.to(device)\n",
    "net = net.eval()\n",
    "net = net.train(False)\n",
    "\n",
    "\n",
    "# for k,v in net.state_dict().items():\n",
    "#     print(k,v.shape)\n",
    "#     if 'conv' in k:\n",
    "#         print(v.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape = torch.Size([8, 3, 160, 320])\n",
      "Result shape = torch.Size([8, 30, 10, 20])\n",
      "Anchors: \n",
      "[[10.762251 13.063103]\n",
      " [25.158768 42.200066]\n",
      " [19.567272 25.438337]\n",
      " [91.87796  35.945087]\n",
      " [38.639523 69.15513 ]]\n",
      "Output sizes: \n",
      "[20 10]\n"
     ]
    }
   ],
   "source": [
    "image_shape = (160, 320, 3)\n",
    "\n",
    "after_load = preprocessing.numpy_to_torch_iou_params(device)\n",
    "to_anchors_single = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,False,False)\n",
    "# to_anchors_multi = lambda *x: preprocessing.to_anchors_for_iou_loss(*x,True,False)\n",
    "\n",
    "anchors = [\n",
    "    \t[10.762251, 13.063103],\n",
    "        [25.158768, 42.200066],\n",
    "        [19.567272, 25.438337],\n",
    "        [91.87796, 35.945087],\n",
    "        [38.639523, 69.15513]\n",
    "    ],\n",
    "anchors = np.array(anchors).reshape((-1,2))\n",
    "\n",
    "# pass example tensor\n",
    "torch.cuda.empty_cache()\n",
    "tensor = torch.rand((8,3,)+image_shape[:2]).to(device)\n",
    "print(\"Input shape =\",tensor.shape)\n",
    "with torch.no_grad():\n",
    "    result = net(tensor)\n",
    "print(\"Result shape =\",result.shape)\n",
    "\n",
    "# get yolo paremeters\n",
    "output_sizes = np.array(result.shape[2:][::-1])\n",
    "del tensor\n",
    "del result\n",
    "\n",
    "print(\"Anchors: \")\n",
    "print(anchors) \n",
    "print(\"Output sizes: \")\n",
    "print(output_sizes) \n",
    "\n",
    "# CREATE GENERATORS\n",
    "def numpy_to_tensor(X,y,device=device):\n",
    "    return utils.data_to_tensor_v3(X,y,device)\n",
    "\n",
    "val_generator = preprocessing.YoloDataGenerator(\n",
    "                            dataset_local_path,\n",
    "                            input_shape=image_shape,\n",
    "                            anchors=anchors,\n",
    "                            images_labes=[], \n",
    "                            batch_size=batch_size,\n",
    "                            name='ValGenerator', \n",
    "                            augmentator=None,\n",
    "                            output_size=output_sizes,\n",
    "                            after_load=after_load,\n",
    "                            bbox_to_anchors=to_anchors_single,\n",
    "                            )\n",
    "test_generator = preprocessing.YoloDataGenerator(\n",
    "                            dataset_local_path,\n",
    "                            input_shape=image_shape,\n",
    "                            anchors=anchors,\n",
    "                            images_labes=[], \n",
    "                            batch_size=batch_size,\n",
    "                            name='TestGenerator', \n",
    "                            augmentator=None,\n",
    "                            output_size=output_sizes,\n",
    "                            after_load=after_load,\n",
    "                            bbox_to_anchors=to_anchors_single,\n",
    "                            )\n",
    "\n",
    "_, val_set = folds.__getitem__(0, train_folds=4)\n",
    "val_generator.images_labes = val_set\n",
    "test_generator.images_labes = folds.test_set[:3000]\n",
    "\n",
    "# decorator -> reorder channels before metric calculation \n",
    "metric_iou = metrics.SingleObjectIOUsBasedMetrics(anchors, image_shape, device)\n",
    "def mean_iou(y_pred:torch.Tensor, y_ref, metric_iou=metric_iou):\n",
    "    # possible reorder\n",
    "    y_pred = y_pred.view(-1,5,6,10,20).permute(0,2,1,3,4).reshape(-1,30,10,20)\n",
    "    # y_pred = y_pred[:,:25,...].contiguous()\n",
    "    y_pred = torch.cat([y_pred[:,20:25,...],y_pred[:,:20,...]],dim=1)\n",
    "    \n",
    "    return metric_iou(y_pred, y_ref)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QFuGuk2NB4fz",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNeeded to run of quantize.\\nfirst with quant_mode = 'calib'\\nsecond with quant_mode = 'test'\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def evaluate(model,\n",
    "             dataloader,\n",
    "             evaluator\n",
    "             ):\n",
    "    with torch.no_grad():\n",
    "        score = 0.0\n",
    "        cntr = 0\n",
    "        for i in range(len(dataloader)):\n",
    "            XY = dataloader[i]\n",
    "            X = XY[0]*2-1\n",
    "            Y = XY[1]\n",
    "            L = X.shape[0]\n",
    "            y_pred = model(X)\n",
    "            score = score*cntr + X.shape[0]*evaluator(y_pred, Y)\n",
    "            cntr += X.shape[0]\n",
    "            score /= cntr\n",
    "            print(\"\\rEvaluation {}/{}. Score = {}\".format(i,len(dataloader), score),end='')\n",
    "        \n",
    "        print(\"\\rEvaluation {}/{}. Score = {}\".format(len(dataloader),len(dataloader), score),end='\\n')\n",
    "\n",
    "\n",
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device,\n",
    "             dataloader,\n",
    "             evaluator):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader target is not needed - for 'calib' must be batch_size == 1\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    # available in docker or after packaging \n",
    "    # vitis-AI-tools/..../pytorch../pytorch_nndct\n",
    "    # and installing the package\n",
    "    from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "    # model to device\n",
    "    model = float_model.to(device)\n",
    "    \n",
    "    # That was present in vai tutorial.\n",
    "    # I don't know if it affects to anything?\n",
    "    # Force to merge BN with CONV for better quantization accuracy\n",
    "    optimize = 1\n",
    "\n",
    "    rand_in = torch.randn((1,)+input_shape[-1:]+input_shape[:2])\n",
    "    print(\"get qunatizer start\")\n",
    "    try:\n",
    "        quantizer = torch_quantizer(\n",
    "            quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\")\n",
    "        print(e)\n",
    "        return\n",
    "    print(\"get qunatizer end\")\n",
    "        \n",
    "    print(\"get quantized model start\")\n",
    "    quantized_model = quantizer.quant_model\n",
    "    print(\"get quantized model end\")\n",
    "\n",
    "    # evaluate\n",
    "    print(\"testing st\")\n",
    "    evaluate(quantized_model, dataloader, evaluator)\n",
    "    print(\"testing end\")\n",
    "\n",
    "    # export config\n",
    "    if quant_mode == 'calib':\n",
    "        print(\"export config\")\n",
    "        quantizer.export_quant_config()\n",
    "        print(\"export config end\")\n",
    "    # export model\n",
    "    if quant_mode == 'test':\n",
    "        print(\"export xmodel\")\n",
    "        quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "        print(\"export xmodel end\")\n",
    "\n",
    "    return\n",
    "\n",
    "\"\"\"\n",
    "Needed to run of quantize.\n",
    "first with quant_mode = 'calib'\n",
    "second with quant_mode = 'test'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only subset of val set\n",
    "# set whole dataset\n",
    "val_generator.images_labes = val_set\n",
    "# shuffle samples\n",
    "val_generator.on_epoch_end()\n",
    "# get subset (100) of samples\n",
    "subset = val_generator.images_labes[:200]\n",
    "val_generator.images_labes = subset\n",
    "# process only one image per forward\n",
    "val_generator.batch_size = 1\n",
    "test_generator.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation 3000/3000. Score = 0.7209295767936862\n",
      "Evaluation 200/200. Score = 0.7442172173783184\n"
     ]
    }
   ],
   "source": [
    "# Evaluate float model on test dataset\n",
    "# test_generator.images_labes = ds['Bird1']\n",
    "evaluate(net,test_generator,evaluator=mean_iou)\n",
    "# Evaluate float model on val dataset\n",
    "evaluate(net,val_generator,evaluator=mean_iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Parsing Sequential...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quantizable module is generated.(quant_dir/Sequential.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 200/200. Score = 0.7024353872239587\n",
      "testing end\n",
      "export config\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Exporting quant config.(quant_dir/quant_info.json)\u001b[0m\n",
      "export config end\n"
     ]
    }
   ],
   "source": [
    "# Quantize model - calib\n",
    "quantize(net, \n",
    "         image_shape,\n",
    "         quant_dir='quant_dir',\n",
    "         quant_mode='calib',\n",
    "         device=device,\n",
    "         dataloader=val_generator,\n",
    "         evaluator=mean_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Parsing Sequential...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Quantizable module is generated.(quant_dir/Sequential.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 3000/3000. Score = 0.6688202833534429\n",
      "testing end\n",
      "export xmodel\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Converting to xmodel ...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[NNDCT_NOTE]: =>Successfully convert 'Sequential' to xmodel.(quant_dir/Sequential_int.xmodel)\u001b[0m\n",
      "export xmodel end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # Quantize model - test\n",
    "    quantize(net, \n",
    "             image_shape,\n",
    "             quant_dir='quant_dir',\n",
    "             quant_mode='test',\n",
    "             device=device,\n",
    "#              dataloader=val_generator,\n",
    "             dataloader=test_generator,\n",
    "             evaluator=mean_iou)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "except:\n",
    "    print(\"XD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[UNILOG][INFO] The compiler log will be dumped at \"/tmp/vitis-ai-user/log/xcompiler-20220223-090245-285893\"\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: function\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_CUSTOMIZED\n",
      "[UNILOG][INFO] Graph name: Sequential, with op num: 156\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/TRAIN/Vitis_AI_FINN/build_test/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/TRAIN/Vitis_AI_FINN/build_test/FINN_VAI.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 33510f34a2ed2650c4bf52b080448e01, and been saved to \"/workspace/TRAIN/Vitis_AI_FINN/build_test/md5sum.txt\"\n"
     ]
    }
   ],
   "source": [
    "!vai_c_xir --xmodel quant_dir/Sequential_int.xmodel --arch arch.json --net_name FINN_VAI --output_dir  build_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images 3000 / 3000\n",
      "Groundtruth is saved under: eval_images_finn/gt.json\n"
     ]
    }
   ],
   "source": [
    "# CREATE TEST EVAL SET\n",
    "subset = folds.test_set[:3000]\n",
    "d = {}\n",
    "generator = preprocessing.YoloDataGenerator(\n",
    "                            dataset_local_path,\n",
    "                            input_shape=image_shape,\n",
    "                            anchors=anchors,\n",
    "                            images_labes=subset, \n",
    "                            batch_size=1,\n",
    "                            name='Generator', \n",
    "                            augmentator=None,\n",
    "                            output_size=output_sizes,\n",
    "                            after_load=after_load,\n",
    "                            bbox_to_anchors=to_anchors_single,\n",
    "                            )\n",
    "\n",
    "for i in range(len(generator)):\n",
    "    print(\"\\rImages\",i,'/',len(generator),end='')\n",
    "    img,Y = generator[i]\n",
    "    img = img.reshape(3,-1).numpy().T.reshape((160,320,3))\n",
    "    img = (img*255).astype(np.uint8)\n",
    "    bbox = np.round(Y[0].reshape(-1).numpy()).astype(int).flatten().tolist()\n",
    "    \n",
    "    new_path = 'images/img_'+(str(i).zfill(4))+'.png'\n",
    "    dst = os.path.join('eval_images_finn/'+new_path)\n",
    "    bbox = {k:v for k,v in zip('ltrb',bbox)}\n",
    "    d[str(i)] = {'path':new_path,'bbox':bbox}\n",
    "    cv.imwrite(dst, img)\n",
    "    \n",
    "print(\"\\rImages\",len(generator),'/',len(generator))\n",
    "# print(d)\n",
    "with open('eval_images_finn/gt.json','w') as f:\n",
    "    f.write(json.dumps(d, indent=4,))\n",
    "    print('Groundtruth is saved under:','eval_images_finn/gt.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Quantize.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
